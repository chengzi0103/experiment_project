---
tags:
  - AutoGen
  - 说明
Create Time: 2024-04-22T14:09:00
Update Time: "{{date}},{{time}}"
Title:
---
# 1，样式案例

## 1.1, 一个简单agent案例

**目标**: **生成一个 NVDA 和 TESLA 2021年至今股价变化图,按照png的格式保存到本地**


~~~/python
  
from autogen import AssistantAgent, UserProxyAgent  
import autogen  
  
from extra.autogen.log_token import load_log_token, create_sqlite_db_name  
  
log_db_name = create_sqlite_db_name()  
logging_session_id = autogen.runtime_logging.start(config={"dbname": log_db_name})  
  
deepseek_llm_config = {"model": "deepseek-chat", "api_key": 'sk-e64f21fb757148a59a8a2edc23094ab2','base_url':'https://api.deepseek.com/v1'}  
yi_llm_config = {"model": "yi-34b-chat-0205", "api_key": '1f2808c63ec94c369999da6e3b13056c',"base_url":"https://api.lingyiwanwu.com/v1"}  
openai_llm_config = {"model": "gpt-4", "api_key": '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'}  
zhipu_llm_config = {"model": "glm-4", "api_key": '094eaeab9c33bacde4365f834e4f259a.TGRRXWqpYE3FPyFx','base_url':'https://open.bigmodel.cn/api/paas/v4'}  
  
assistant = AssistantAgent("assistant", llm_config={"config_list": [openai_llm_config]},human_input_mode='NEVER')  
user_proxy = UserProxyAgent("user_proxy", code_execution_config={"work_dir": "/mnt/d/project/dy/extra/autogen/output","use_docker":False},)  
user_proxy.initiate_chat(assistant, message="生成一个 NVDA 和 TESLA 2021年至今股价变化图,按照png的格式保存到本地")  
autogen.runtime_logging.stop()  
print(load_log_token(logging_session_id=logging_session_id,dbname=log_db_name, table="chat_completions"))
~~~

**token计算**:
~~~/json
{'total_tokens': '3249', 'total_cost': '0.1165', 'session_id': 'd3c7dafd-6e7d-4ee0-8705-1461dfd94b5d'}

~~~

### AssistantAgent参数详解

`AssistantAgent` 是一个可以与用户或其他代理进行交互的代理。它通常用于执行任务，如回答问题、提供信息或执行特定的命令。

- `name`: 这是代理的名称，用于标识和区分不同的代理。
- `llm_config`: 这是一个配置字典，用于指定大型语言模型（LLM）的配置。在这个例子中，`llm_config` 包含一个键 `"config_list"`，其值是一个列表，列表中包含了一个名为 `zhipu_llm_config` 的配置项。这个配置项可能包含了与特定语言模型相关的参数，如模型类型、API密钥等。

### UserProxyAgent

`UserProxyAgent` 是一个代理，它代表用户并与 `AssistantAgent` 交互。它通常用于模拟用户输入或执行用户请求的任务。

- `name`: 这是代理的名称，用于标识和区分不同的代理。
- `code_execution_config`: 这是一个配置字典，用于指定代码执行的配置。在这个例子中，`code_execution_config` 包含以下键：
  - `work_dir`: 这是代码执行的工作目录，用于存储临时文件或输出结果。
  - `use_docker`: 这是一个布尔值，指示是否使用 Docker 容器来执行代码。在这个例子中，`use_docker` 被设置为 `False`，这意味着代码将在本地环境中执行，而不是在 Docker 容器中。

### initiate_chat 方法

`initiate_chat` 是 `UserProxyAgent` 类的一个方法，用于启动与 `AssistantAgent` 的对话。

- `assistant`: 这是与 `UserProxyAgent` 对话的 `AssistantAgent` 实例。
- `message`: 这是发送给 `AssistantAgent` 的初始消息。在这个例子中，消息是 "绘制 NVDA 和 TESLA 年初至今股价变化图"，这意味着用户请求代理生成一个图表，显示 NVIDIA (NVDA) 和 Tesla (TESLA) 股价从年初到现在的变化情况。

综上所述，这段代码创建了两个代理：一个用于执行任务的 `AssistantAgent` 和一个用于与用户交互的 `UserProxyAgent`。`UserProxyAgent` 使用 `initiate_chat` 方法启动与 `AssistantAgent` 的对话，并请求生成一个图表来展示 NVDA 和 TESLA 股价的变化。这些代理的配置允许它们在本地环境中执行代码，而不是在 Docker 容器中。



## 1.2 Sequential Chats (顺序总结聊天)


**例子说明**:
- 需要构建一个计算器,对某个数字进行加减乘除
- 一个“Number_Agent”的代理负责提出一个数字，其他agent负责对数字执行特定的算术运算，例如，加 1、乘以 2 等。


**顺序总结聊天说明**:
- 在此模式中，一对agent首先开始聊天，然后对话的摘要成为下一个双agent聊天的延续。下一个聊天将结转传递给上下文的 `carryover` 参数以生成其初始消息。
- 随着对话的进行，结果会累积，因此每个后续聊天都从之前聊天的所有输出开始。
- 此模式对于可以分解为相互依赖的子任务的复杂任务非常有用。下图说明了此模式的工作原理。

![[Sequential_Chats.png]]


~~~/python
  
import os  
import autogen  
from autogen import ConversableAgent  
from extra.autogen.log_token import create_sqlite_db_name, load_log_token  
  
# The Number Agent always returns the same numbers.  
qwn_32b_llm_config = {"model": "qwen:32b", "api_key": 'ollama',"base_url":"http://127.0.0.1:11434/v1"}  
qwn_16b_llm_config = {"model": "qwen:14b", "api_key": 'ollama',"base_url":"http://192.168.0.11:11434/v1"}  
llama3_16b_llm_config = {"model": "llama3:8b", "api_key": 'ollama',"base_url":"http://192.168.0.11:11434/v1"}  
yi_llm_config = {"model": "yi-34b-chat-0205", "api_key": '1f2808c63ec94c369999da6e3b13056c',"base_url":"https://api.lingyiwanwu.com/v1"}  
openai_llm_config = {"model": "gpt-4", "api_key": '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'}  
zhipu_llm_config = {"model": "glm-4", "api_key": '094eaeab9c33bacde4365f834e4f259a.TGRRXWqpYE3FPyFx','base_url':'https://open.bigmodel.cn/api/paas/v4'}  
deepseek_llm_config = {"model": "deepseek-chat", "api_key": 'sk-e64f21fb757148a59a8a2edc23094ab2','base_url':'https://api.deepseek.com/v1'}  
llm_config_list = [qwn_32b_llm_config,llama3_16b_llm_config,yi_llm_config,openai_llm_config,zhipu_llm_config,deepseek_llm_config]  
llm_config_list = [openai_llm_config]  
log_db_name = create_sqlite_db_name()  
logging_session_id = autogen.runtime_logging.start(config={"dbname": log_db_name})  
number_agent = ConversableAgent(  
    name="Number_Agent",  
    system_message="You return me the numbers I give you, one number each line.",  
    # 你把我给你的数字还给我，每行一个数字。  
    llm_config={"config_list": llm_config_list},  
    human_input_mode="NEVER",  
)  
  
# The Adder Agent adds 1 to each number it receives.  
adder_agent = ConversableAgent(  
    name="Adder_Agent",  
    system_message="You add 1 to each number I give you and return me the new numbers, one number each line.",  
    # 你给我的每个数字加 1，然后把新数字还给我，每行一个数字。  
    llm_config={"config_list": llm_config_list},  
    human_input_mode="NEVER",  
)  
  
# The Multiplier Agent multiplies each number it receives by 2.  
multiplier_agent = ConversableAgent(  
    name="Multiplier_Agent",  
    system_message="You multiply each number I give you by 2 and return me the new numbers, one number each line.",  
    # 你把我给你的每个数字乘以 2，然后把新数字还给我，每行一个数字。  
    llm_config={"config_list": llm_config_list},  
    human_input_mode="NEVER",  
)  
  
# The Subtracter Agent subtracts 1 from each number it receives.  
subtracter_agent = ConversableAgent(  
    name="Subtracter_Agent",  
    system_message="You subtract 1 from each number I give you and return me the new numbers, one number each line.",  
    # 你从我给你的每个数字中减去 1，然后把新数字还给我，每行一个数字。  
    llm_config={"config_list": llm_config_list},  
    human_input_mode="NEVER",  
)  
  
# The Divider Agent divides each number it receives by 2.  
divider_agent = ConversableAgent(  
    name="Divider_Agent",  
    system_message="You divide each number I give you by 2 and return me the new numbers, one number each line.",  
    # 你把我给你的每个数字除以 2，然后把新数字还给我，每行一个数字。  
    llm_config={"config_list": llm_config_list},  
    human_input_mode="NEVER",  
)  
  
chat_results = number_agent.initiate_chats(  
    [  
        {  
            "recipient": adder_agent,  
            "message": "14",  
            "max_turns": 1,  
            "summary_method": "last_msg",  
        },  
        {  
            "recipient": multiplier_agent,  
            "message": "These are my numbers",  
            "max_turns": 1,  
            "summary_method": "last_msg",  
        },  
        {  
            "recipient": subtracter_agent,  
            "message": "These are my numbers",  
            "max_turns": 1,  
            "summary_method": "last_msg",  
        },  
        {  
            "recipient": divider_agent,  
            "message": "These are my numbers",  
            "max_turns": 1,  
            "summary_method": "last_msg",  
        },  
    ]  
)  
autogen.runtime_logging.stop()  
print(load_log_token(logging_session_id=logging_session_id,dbname=log_db_name, table="chat_completions"))
~~~


**token数量计算**:
~~~
{'total_tokens': '596', 'total_cost': '0.0191', 'session_id': 'ec145021-abb4-4884-bf49-14e581e986d0'}
~~~


**出现的问题**:
输出的结果和示例结果不正确. 发现是 模型的问题, 模型不能正常识别任务,就会出现上下文冲突的问题,当前只使用openai和zhipu才能正常走通这个示例,其他的ai-api都不能正常运行

~~~
Please provide the next number you would like me to add 1 to.
Apologies for the confusion. If you provide me with a number, I will multiply it by 2 and give you the result. Please give me a number, and I will start.
I apologize for the confusion earlier. If you give me a number, I will subtract 1 from it and give you the new result. Please provide the first number.

--------------------------------------------------------------------------------
Divider_Agent (to Number_Agent):

I apologize for the confusion. It seems there was a misunderstanding. If you provide me with a number, I will divide it by 2 and give you the result. Please provide the first number, and I will do as you requested.

~~~

**来源**:
- https://microsoft.github.io/autogen/docs/tutorial/conversation-patterns



## 1.3 GroupChat 群聊

**例子说明**:
- 将 [病人/挂号/护士/医生/药房/收费处]等几个agent代理放到一个群聊中,模拟病人因为生病去医院就诊的流程.
- 就医流程为: 挂号->就诊->缴费->取药
- 案例的主要作用是通过GroupChat群聊,如何快速的将多个不同的agent来实现同一个目标(尤其是agent是两个或者两个以上)
### GroupChat 类

`GroupChat` 类是一个预览版的群聊类，包含以下数据字段：

- **agents**: 参与的代理列表。
- **messages**: 群聊中的消息列表。
- **max_round**: 最大轮数。
- **admin_name**: 管理员代理的名称，如果有的话。默认为 "Admin"。`KeyBoardInterrupt` 将使管理员代理接管。
- **func_call_filter**: 是否强制执行函数调用过滤。默认为 True。当设置为 True 且消息是函数调用建议时，下一个发言者将从包含相应函数名称在其 `function_map` 中的代理中选择。
- **select_speaker_message_template**: 自定义选择发言者消息（用于“自动”发言者选择），它首先出现在消息上下文中，通常包括代理描述和代理列表。字符串值将转换为 f-string，使用 "{roles}" 输出代理及其角色描述，"{agentlist}" 用于方括号中的代理名称的逗号分隔列表。默认值为："You are in a role play game. The following roles are available: {roles}. Read the following conversation. Then select the next role from {agentlist} to play. Only return the role."
- **select_speaker_prompt_template**: 自定义选择发言者提示（用于“自动”发言者选择），它最后出现在消息上下文中，通常包括代理列表和指导LLM选择下一个代理的指南。字符串值将转换为 f-string，使用 "{agentlist}" 为方括号中的代理名称的逗号分隔列表。默认值为："Read the above conversation. Then select the next role from {agentlist} to play. Only return the role."
- **speaker_selection_method**: 选择下一个发言者的方法。默认为 "auto"。可以是以下任何一种（不区分大小写），如果未被识别将引发 ValueError：
  - "auto": 下一个发言者由LLM自动选择。
  - "manual": 下一个发言者由用户输入手动选择。
  - "random": 下一个发言者随机选择。
  - "round_robin": 下一个发言者以循环方式选择，即按照提供的 `agents` 中的顺序迭代。
  - 自定义发言者选择函数（Callable）：该函数将被调用以选择下一个发言者。函数应该接受最后一个发言者和群聊作为输入，并返回以下之一：
    1. `Agent` 类，它必须是群聊中的一个代理。
    2. 字符串，从 ['auto', 'manual', 'random', 'round_robin'] 中选择一个默认方法使用。
    3. None，这将优雅地终止对话。
- **allow_repeat_speaker**: 是否允许相同的发言者连续发言。默认为 True，在这种情况下，所有发言者都被允许连续发言。如果 `allow_repeat_speaker` 是代理的列表，则只有那些列出的代理被允许重复。如果设置为 False，则不允许任何发言者重复。`allow_repeat_speaker` 和 `allowed_or_disallowed_speaker_transitions` 是互斥的。
- **allowed_or_disallowed_speaker_transitions**: 字典。键是源代理，值是键代理可以/不能转移到的代理，取决于 speaker_transitions_type。默认为 None，这意味着所有代理都可以转移到所有其他代理。`allow_repeat_speaker` 和 `allowed_or_disallowed_speaker_transitions` 是互斥的。
- **speaker_transitions_type**: `speaker_transitions_type` 是一个包含允许代理或不允许代理的字典。"allowed" 表示 `allowed_or_disallowed_speaker_transitions` 是一个包含允许代理的字典。如果设置为 "disallowed"，则 `allowed_or_disallowed_speaker_transitions` 是一个包含不允许代理的字典。如果 `allowed_or_disallowed_speaker_transitions` 不为 None，则必须提供。
- **enable_clear_history**: 启用手动为代理清除消息历史的可能性，通过在用户提示中提供 "clear history" 短语实现。这是一个实验性功能。有关 GroupChatManager.clear_agents_history 函数的更多信息，请参见描述。
- **send_introductions**: 在群聊开始时发送一轮介绍，以便代理知道他们可以与谁交谈（默认：False）
- **role_for_select_speaker_messages**: 设置在 'auto' 模式下选择发言者消息的角色名，通常为 'user' 或 'system'。（默认：'system'）

此外，还包括一些方法，如 `agent_names`, `reset`, `append`, `agent_by_name`, `nested_agents`, `next_agent`, `select_speaker_msg`, `select_speaker_prompt`, `introductions_msg`, `manual_select_speaker`, `random_select_speaker`, 和 `select_speaker`，这些方法提供了操作群聊的不同方式和功能。



~~~
import autogen  
from autogen import AssistantAgent, GroupChat, GroupChatManager  
from extra.autogen.log_token import load_log_token, create_sqlite_db_name  
  
import random  
log_db_name = create_sqlite_db_name()  
  
qwn_32b_llm_config = {"model": "qwen:32b", "api_key": 'ollama' ,"base_url" :"http://127.0.0.1:11434/v1"}  
qwn_16b_llm_config = {"model": "qwen:14b", "api_key": 'ollama' ,"base_url" :"http://192.168.0.11:11434/v1"}  
llama3_16b_llm_config = {"model": "llama3:8b", "api_key": 'ollama' ,"base_url" :"http://192.168.0.11:11434/v1"}  
yi_llm_config = {"model": "yi-34b-chat-0205", "api_key": ''  
                 ,"base_url" :"https://api.lingyiwanwu.com/v1"}  
openai_llm_config = {"model": "gpt-4", "api_key": ''}  
zhipu_llm_config = {"model": "glm-4", "api_key": ''  
                    ,'base_url' :'https://open.bigmodel.cn/api/paas/v4'}  
deepseek_llm_config = {"model": "deepseek-chat", "api_key": ''  
                       ,'base_url' :'https://api.deepseek.com/v1'}  
llm_config_list = [qwn_32b_llm_config  ,yi_llm_config ,openai_llm_config ,zhipu_llm_config  
                   ,deepseek_llm_config]  
logging_session_id = autogen.runtime_logging.start(config={"dbname": log_db_name})  
  
# 患者代理  
user_agent = AssistantAgent(  
   name="liming",  
   system_message='你是一个人类，姓名：橙子，25岁，男，昨天打球时脚崴了',  
    llm_config={"config_list": [random.choice(llm_config_list)]},  
    human_input_mode="NEVER",  
)  
  
# 挂号代理  
registered_agent = AssistantAgent(  
    name="registered",  
    system_message="你是医院的挂号系统，患者来到医院，引导患者完成挂号。需要根据患者选择的科室推荐对应科室的医生及收费情况，当患者选择对应医生并且输入对应医生的挂号费用时，告诉患者:挂号完成，并可前往诊室候诊 ",  
    llm_config={"config_list": [random.choice(llm_config_list)]},  
    human_input_mode="NEVER",  
    # function_map={  
    #     "get_doctor_info": get_doctor_info    # })  
  
# 医生代理，可以创建多个  
doctor_zhao_agent = AssistantAgent(  
    name="doctor_zhao",  
    system_message="你是一名骨科医生，名字赵六，只接收挂号完成的患者，应该介绍自己然后询问患者病情，根据患者病情给予合适的治疗方案,并开具对应的药品处方",  
    llm_config={"config_list": [random.choice(llm_config_list)]},  
    human_input_mode="NEVER"  
)  
  
# 药房代理  
medicine_agent = AssistantAgent(  
    name="medicine_store",  
    system_message="你是一名药房管理员，根据用户的处方及缴费信息给用户出售对应的药品，当用户未缴费时提醒用户先去缴费窗口进行缴费",  
    llm_config={"config_list": [random.choice(llm_config_list)]},  
    human_input_mode="NEVER"  
)  
  
# 收费代理  
cashier_agent = AssistantAgent(  
    name="cashier",  
    system_message="你是一名药品收费员，根据用户的处方收取对应药品的费用，当用户输入费用时,代表用户缴费完成,并开具收据，提醒用户取药房取药",  
    llm_config={"config_list": [random.choice(llm_config_list)]},  
    human_input_mode="NEVER"  
)  
group_chat = GroupChat(  
    agents=[user_agent, registered_agent, doctor_zhao_agent, medicine_agent, cashier_agent],  
    messages=[],  
    max_round=20,  
    send_introductions=True  
)  
  
group_chat_manager = GroupChatManager(  
    groupchat=group_chat,  
    llm_config={"config_list": [openai_llm_config]},  
)  
  
user_agent.initiate_chat(  
    group_chat_manager,  
    message="你好,我的脚受伤了",  
    summary_method="last_msg",  
    silent=True  
)  
  
autogen.runtime_logging.stop()  
print(load_log_token(logging_session_id=logging_session_id,dbname=log_db_name, table="chat_completions"))
~~~


**token数量**:
~~~/json
{'total_tokens': '40166', 'total_cost': '0.959', 'session_id': 'c8677a24-3744-4185-9268-c4e4c7dc3347'}

#加上护士角色后的费用
{'total_tokens': '114407', 'total_cost': '2.8635', 'session_id': 'fcc7e127-0cf3-41b5-89cb-dc27e14fc3d9'}


~~~

## 3.4, 基于Graph的自定义多Agents交流

### 3.4.1 案例一 

**案例背景**:
假设在一家公司里，有五个部门，每个部门有一个代表，这些代表需要就某个议题进行讨论并最终做出决策。这五个部门的代表分别是：总经理（Agent0）、销售部门（Agent1）、市场部门（Agent2）、研发部门（Agent3）和人力资源部门（Agent4）。

**案例实现逻辑**：
- 总经理（Agent0）有权启动讨论，并可以选择任何部门开始发言。
一旦某个部门（Agent1到Agent4）发言完毕，他们必须将发言权交回给总经理（Agent0），由总经理决定下一个发言的部门。
- 这种设置确保了讨论的有序进行，并给予了总经理控制讨论流程的能力，以便更有效地达成决策。

**为什么要这样做?**
- 确保决策过程的有效性：通过让总经理控制发言权的转移，可以确保每个部门都有机会表达自己的观点，同时避免了会议中可能出现的混乱和重复讨论。
- 提高会议效率：这种发言权转换机制可以加快会议进程，因为总经理可以根据讨论的需要灵活地安排发言顺序，确保讨论重点和高效进行。
- 可视化决策路径：通过可视化工具，所有参与者都能清楚地了解讨论的流程和规则，有助于提高透明度和参与感。

![[graph_example_01.png]]

~~~
from autogen.graph_utils import visualize_speaker_transitions_dict  
from autogen import ConversableAgent  
  
agents = [ConversableAgent(name=f"Agent{i}", llm_config=False) for i in range(5)]  
allowed_speaker_transitions_dict = {  
    agents[0]: [agents[1], agents[2], agents[3], agents[4]],  
    agents[1]: [agents[0]],  
    agents[2]: [agents[0]],  
    agents[3]: [agents[0]],  
    agents[4]: [agents[0]],  
}  
  
visualize_speaker_transitions_dict(allowed_speaker_transitions_dict, agents)
~~~


### 3.4.2 案例二

**案例背景**:
假设在一个公司中，有三个部门：产品部门（A）、市场部门（B）和技术部门（C）。每个部门都有五名成员，包括一名部门领导和四名普通成员。为了促进部门内部以及部门间的有效沟通，我们需要建立一个清晰的沟通框架


**案例实现逻辑**：
-  团队内部沟通：首先，每个部门内部的成员需要能够自由地相互沟通，以便于分享信息、协调工作和解决问题。代码中通过循环为每个成员设置了与本部门其他成员的连接，确保了部门内部的流畅沟通。
- 部门间的领导沟通：为了协调不同部门之间的工作，各部门的领导需要有一个直接沟通的渠道。代码中特别为产品部门领导（A0）、市场部门领导（B0）和技术部门领导（C0）设置了相互连接，使得他们可以直接交流，从而促进跨部门的协作和决策。

**为什么要这样做?**
- 提高效率：清晰的沟通路径减少了信息传递的混乱和延误，提高了工作效率。
- 优化协作：促进了团队内部和跨团队的协作，有助于解决跨部门协作中可能出现的障碍。
- 增强透明度：通过可视化沟通路径，所有团队成员都能清楚地了解沟通结构和流程，提高了组织内部的透明度和理解。

![[graph_example_02.png]]
~~~
from autogen.graph_utils import visualize_speaker_transitions_dict  
from autogen import ConversableAgent
speaker_transitions_dict = {}  
teams = ["A", "B", "C"]  
team_size = 5  
  
  
def get_agent_of_name(agents, name) -> ConversableAgent:  
    for agent in agents:  
        if agent.name == name:  
            return agent  
  
  
# Create a list of 15 agents 3 teams x 5 agents  
agents = [ConversableAgent(name=f"{team}{i}", llm_config=False) for team in teams for i in range(team_size)]  
  
# Loop through each team and add members and their connections  
for team in teams:  
    for i in range(team_size):  
        member = f"{team}{i}"  
        # Connect each member to other members of the same team  
        speaker_transitions_dict[get_agent_of_name(agents, member)] = [  
            get_agent_of_name(agents, name=f"{team}{j}") for j in range(team_size) if j != i  
        ]  
  
# Team leaders connection  
print(get_agent_of_name(agents, name="B0"))  
speaker_transitions_dict[get_agent_of_name(agents, "A0")].append(get_agent_of_name(agents, name="B0"))  
speaker_transitions_dict[get_agent_of_name(agents, "B0")].append(get_agent_of_name(agents, name="C0"))  
  
visualize_speaker_transitions_dict(speaker_transitions_dict, agents)
~~~




# 4, 需要注意的问题以及解决方案

## 4.1 Token超出模型最大值
当我们使用autogen的时候,可能会出现某个agent会将上下文带入到自己的问题中,从而导致token爆炸
~~~
openai.BadRequestError: Error code: 400 - {'error': {'code': 'bad_request', 'message': 'The total number of tokens in the prompt and the max_tokens must be less than or equal to 4096. The prompt has 3790 tokens and the max_tokens is 512 tokens.', 'type': 'invalid_request_error', 'param': None}}

~~~

## 4.2 中英文描述
当我们使用中文和英文去描述每个Assistant和需求的过程中,模型对中文和英文的理解能力不同,大部分情况对英文的理解能力会更强,但是更重要的是你需要说明你的功能和问题

## 4.3 Openai-api 超出最大tpm范围
多次运行的过程中,出现这个问题
出现问题的情况
~~~
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-6Ic7K1WX3cvccMoyzfCVNHue on tokens per min (TPM): Limit 10000, Used 7819, Requested 5699. Please try again in 21.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
ERROR conda.cli.main_run:execute(124): `conda run python /mnt/d/project/dy/extra/autogen/example_03.py` failed. (See above for error)
~~~

## 4.5 运行一个任务,结果完全不相同
这个问题比较复杂,内部的原理不太明白,但是通过选择效果较好的模型是可以完全避免这个问题的,怀疑是多个agent上下文出现了问题

~~~
选择效果较好的模型可以改变这个问题
~~~



## 4.6 Token使用量统计
使用全网最便宜的  **"零一万物"** 模型api来进行狼人杀agent,以下是消费账单,会发现大概3论会话,会消耗1.5W的token
![[token消费.png]]


## 4.7 如何节省Token,减少Api的费用

**LLM Caching**:AutoGen 支持缓存 API 请求，以便在发出相同请求时可以重复使用它们。这在重复或继续实验以实现可重复性和节省成本时很有用。

使用以下方法节省Token:
- https://microsoft.github.io/autogen/docs/topics/llm-caching/ 