import json
import random
import time
from pathlib import Path
from typing import Union, List
import dspy
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_postgres import PGVector
from langchain_core.documents import Document

from experiment_project.agents.dspy_module.base_module import TaskAnalysisModule, QualityEnhancerModule, \
    FindTaskKeyWordsModule
from experiment_project.agents.dspy_module.base_signature import init_costar_signature, init_costar_signature_input
from experiment_project.utils.files.split import split_txt_by_langchain
from langchain_community.document_loaders import PyPDFLoader

from experiment_project.utils.rag.embedding.huggingface import load_embedding_model
from experiment_project.utils.rag.vector.pgvector import create_pgvector, upload_files_to_vector, \
    delete_vector_collection, search_vector


class BaseRag(dspy.Module):
    def __init__(self, module_path:str=None,model_name:str=None, pg_connection:str=None, collection_name:str='my_docs', is_upload_file:bool=False, files_path:List[str]=None, model_kwargs:dict={'device':0}, chunk_size:int=256, encoding:str= 'utf-8',multi_process:bool=False):
        super().__init__()
        self.embedding = load_embedding_model(module_path=module_path,model_kwargs=model_kwargs,multi_process=multi_process,model_name=model_name)
        self.vectorstore = create_pgvector(embedding=self.embedding, collection_name=collection_name, pg_connection=pg_connection)
        if is_upload_file is True and files_path is not None:
            upload_files_to_vector(vectorstore=self.vectorstore,files_path=files_path, chunk_size=chunk_size, encoding=encoding)

    def delete_collection(self):
        delete_vector_collection(vectorstore=self.vectorstore)

    def search(self, keywords: Union[List[str],str], k: int = 6):
        return search_vector(vectorstore=self.vectorstore,keywords=keywords,k=k)

    @property
    def no_cache(self):
        return dict(temperature=0.7 + 0.0001 * random.uniform(-1, 1))


class ReasonerRagModule(BaseRag):
    def __init__(self, module_path:str=None,model_name:str=None, pg_connection:str=None, collection_name:str='my_docs', is_upload_file:bool=False, files_path:List[str]=None,encoding:str= 'utf-8',chunk_size:int=256,multi_process:bool=False,rag_search_num:int=5):
        super().__init__(module_path=module_path, pg_connection=pg_connection, collection_name=collection_name, is_upload_file=is_upload_file, files_path=files_path,encoding=encoding,chunk_size=chunk_size,multi_process=multi_process,model_name=model_name)

        self.task_evaluation = FindTaskKeyWordsModule()
        self.quality_enhancer = QualityEnhancerModule()
        self.rag_search_num = rag_search_num

    def forward(self, question:str):
        task_result = json.loads(self.task_evaluation.forward(question=question).answer)
        question = task_result.get('task_description')
        keywords = task_result.get('keywords')
        llm_result = ''
        all_rag_result = self.search(keywords=keywords,k=self.rag_search_num)
        quality_enhancer_result = self.quality_enhancer.forward(question=question,rag_data=json.dumps(all_rag_result),llm_data=llm_result,)
        result = ''
        if quality_enhancer_result.answer == '':
            result = quality_enhancer_result.results
        else:
            result = quality_enhancer_result.answer
        return result

class SelfRefineRagModule(BaseRag):
    def __init__(self,module_path:str=None,model_name:str=None, pg_connection:str=None, collection_name:str='my_docs', is_upload_file:bool=False, files_path:List[str]=None,encoding:str= 'utf-8',chunk_size:int=256,multi_process:bool=False,rag_search_num:int=5,max_iterations:int=3, feedback_prompt:Union[str,None]=None, refinement_prompt:Union[str,None]=None, stop_condition_prompt:Union[str,None]=None, temperature:float=0.7):
        super().__init__(module_path=module_path, pg_connection=pg_connection, collection_name=collection_name,
                         is_upload_file=is_upload_file, files_path=files_path, encoding=encoding, chunk_size=chunk_size,
                         multi_process=multi_process, model_name=model_name)
        self.task_evaluation = FindTaskKeyWordsModule()



# from experiment_project.utils.initial.util import init_sys_env
# from experiment_project.utils.files.read import read_yaml
# import dspy
#
# init_sys_env(proxy_url= 'http://192.168.31.215:10890')
# secret_env_file = '/mnt/c/Users/chenzi/Desktop/project/env_secret_config.yaml'
#
# api_configs = read_yaml(secret_env_file)
#
# model_config = api_configs.get('openai')
# turbo = dspy.OpenAI(model=model_config.get('model'), max_tokens=4096,api_key=model_config.get('api_key'))
# dspy.settings.configure(lm=turbo)
# # module_path = '/mnt/d/models/embeddings/bce-embedding-base_v1'
# module_path = '/mnt/c/Users/chenzi/Desktop/project/model/bce-embedding-base_v1'
# is_upload_file = True
# # files_path = ['/mnt/d/project/experiment_project/data/text_to_kg/三体1疯狂年代.txt','/mnt/c/Users/cc/Desktop/pic/Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering.pdf']
# # files_path = ['/mnt/c/Users/cc/Desktop/pic/Text-Animator Controllable Visual Text Video Generation.pdf']
# files_path = ['/mnt/c/Users/chenzi/Desktop/project/data/妊娠期高血糖诊治指南（2022）［第一部分］.pdf']
# encoding = 'gbk'
# rag_search_num = 10
# collection_name = '2022'
# refine_module = ReasonerRagModule(module_path=module_path,is_upload_file=is_upload_file,files_path=files_path,encoding=encoding,rag_search_num=rag_search_num)
# # result = refine_module.forward(question="Provide a detailed summary of the theme and explanation of the paper 'Text-Animator Controllable Visual Text Video Generation'. Additionally, explain which papers it cites, what achievements it has made, when it was written, and who the authors are.")
# result = refine_module.forward(question="根据这份指南告诉我孕12周空腹血糖5.3mmol/L的孕妇该如何诊断和管理血糖？")
# print(result)

#
# question = "Provide a detailed summary of the theme and explanation of the paper 'Text-Animator Controllable Visual Text Video Generation'. Additionally, explain which papers it cites, what achievements it has made, when it was written, and who the authors are."
# rag_data = [{'summary': ['Frame 1 Frame 5 Frame 10 Frame 15Figure 6. The example of large-area text movement, demonstrates\nthat our method does not cause damage to the text when moving\ntext over a large area.\nSpeed=4.0x\nSpeed=6.0xSpeed=1.0x\nframe 1 frame 5 frame 10\nFigure 7. The comparison of the same text and camera information\nat different speeds. The prompt is ‘A delicious and square cake\nwith the words ‘HAPPY”.\nexperiments, we have demonstrated that Text-Animator\noutperforms existing T2V and hybrid T2I/I2V methods in\nterms of video quality and fidelity of textual representation.\nOur contributions not only address current challenges\nbut also inspire further exploration and innovation in this\nrapidly evolving field of multimedia content generation.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477 , 2023. 3\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2, 6\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh. Video generation models as world simulators.\n2024. 3\n[4] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li,\nChanghua Meng, Huijia Zhu, Weiqiang Wang, et al. Diffute:Universal text editing diffusion model. Advances in Neural\nInformation Processing Systems , 36, 2024. 3\n[5] Ruoyu Guo Xiaoting Yin Kaitao Jiang Yongkun Du Yun-\ning Du Lingfeng Zhu Baohua Lai Xiaoguang Hu Dianhai\nYu Yanjun Ma Chenxia Li, Weiwei Liu. Pp-ocrv3: More at-\ntempts for the improvement of ultra lightweight ocr system.\narXiv preprint arXiv:2206.03001 , 2022. 3\n[6] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Animateanything: Fine-\ngrained open domain image animation with motion guid-\nance. arXiv e-prints , pages arXiv–2311, 2023. 2\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems , 35:16890–16902, 2022. 3\n[8] Gen-2, September 25, 2023. https://research.\nrunwayml.com/gen2 . 6, 7\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 3\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725 , 2023. 3, 5, 6, 7\n[11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling\ncamera control for text-to-video generation. arXiv preprint\narXiv:2404.02101 , 2024. 5\n[12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu,\nKe Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator:\nZero-shot identity-preserving human video generation. arXiv\npreprint arXiv:2404.15275 , 2024. 3\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221 ,\n2022. 3\n[14] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. ICLR , 2014. 3\n[15] Pika labs, 2023. https://www.pika.art/ . 6, 7\n[16] Lykon, 2023. https://huggingface.co/Lykon/\ndreamshaper-8 . 5\n[17] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,\nHaonan Lu, and Xiaodong Lin. Glyphdraw: Learning to\ndraw chinese characters in image synthesis models coher-\nently. arXiv preprint arXiv:2303.17870 , 2023. 2, 3',
#    'tent. Besides, in Fig. 5, we show one example of the\nLAION-subset dataset. Only our method can correctly dis-\nplay the visual characters (CHRISTMAS) and the number\nof bags (two).\nAt the same time, we also conducted experiments to ver-\nify the robustness of our method. In Fig. 6, we demonstrate\nthe robustness of our method for large movement in the text\nregion. The existing SOTA methods deformed the text area\nduring small movements (as shown in the example above),\nso the visualization results of these methods are not shown\nhere. The texts for these two examples are ‘A coffee mug\nwith the words ‘cafe’ on the office desk’ and ‘A bottom of\nmilk with the words ‘MILK’. The direction of movement is\nfrom right to left. We can see that the structure of our text\ncan still be maintained even with a large range of camera\nmovements. In Fig. 7, we demonstrate that under the same\ncamera information, we can control its movement speed by\nsampling the camera information of the interval frames. At\na speed of 4 or 6 times the original speed, our method is still\nable to maintain the structure of the text.\n4.5. Ablation Study\nIn this part, to illustrate the contributions of our method, we\nconduct ablation studies on LAION-subset. The quantita-\ntive comparisons are shown in Table 3.\nDual control : We conduct an ablation study to ana-\nlyze the effectiveness of the dual control design. Generally\nspeaking, it is feasible to use only position boxes for guid-\nance without using camera poses. Therefore, we designed\nthe ‘W/o camera control’ model, which removed the camera\nguidance module compared to the original model. In addi-\ntion, we removed the position block and only used camera\npose and glyph embedding, and named this model ‘W/o po-',
#    "Text-Animator: Controllable Visual Text Video Generation\nLin Liu1Quande Liu2Shengju Qian2Yuan Zhou3\nWengang Zhou1Houqiang Li1Lingxi Xie4Qi Tian4\n1EEIS Department, University of Science and Technology of China\n2Tencent3Nanyang Technical University\n4Huawei Tech.\nA coffee mug on the office desk with the words ‘CAFE’\nA girl wearing a blue T shirt with ‘beauty’, seaside background\n A bottle of perfume with the words 'CLASSIC'A road sign on the road with the word ‘STOP’\nA raccoon stands in front of a street wall with the words ‘WELCOME’\nOn a street during the night, neon lights spell out ‘ 小吃街 ’\nFigure 1. Given a sentence with visualized words, our Text-Animator is able to produce a wide range of videos that not only show the\nsemantic information of given text prompts, but further align with the visualized words. Our method is a one stage method without further\ntuning.\nAbstract\nVideo generation is a challenging yet pivotal task in var-\nious industries, such as gaming, e-commerce, and adver-\ntising. One significant unresolved aspect within T2V is the\neffective visualization of text within generated videos. De-\nspite the progress achieved in Text-to-Video (T2V) genera-\ntion, current methods still cannot effectively visualize texts\nin videos directly, as they mainly focus on summarizing se-\nmantic scene information, understanding, and depicting ac-\ntions. While recent advances in image-level visual text gen-\neration show promise, transitioning these techniques into\nthe video domain faces problems, notably in preserving tex-tual fidelity and motion coherence. In this paper, we pro-\npose an innovative approach termed Text-Animator for vi-\nsual text video generation. Text-Animator contains a text\nembedding injection module to precisely depict the struc-\ntures of visual text in generated videos. Besides, we develop\na camera control module and a text refinement module to\nimprove the stability of generated visual text by control-\nling the camera movement as well as the motion of visu-\nalized text. Quantitative and qualitative experimental re-\nsults demonstrate the superiority of our approach to the ac-\ncuracy of generated visual text over state-of-the-art video\ngeneration methods. The project page can be found inarXiv:2406.17777v1  [cs.CV]  25 Jun 2024",
#    'laulampaul.github.io/text-animator.html .\n1. Introduction\nVideo generation has become an important cornerstone in\ncontent-based generation, and has huge potential value in\nvarious domains including e-commerce, advertising, the\nfilm industry, etc. For instance, in advertising scenarios,\nit is essential to display a manufacturer’s specific logo or\nslogan in the generated video in the form of text, while also\nseamlessly integrating text with the products featured in the\nvideo (e.g. a piece of clothing). However, in current video\ngeneration approaches, the visualization of text/words in the\ngenerated video remains a challenging yet unresolved is-\nsue. For example, in the first example of Fig. 1, we need\nto engrave the word ”CAFE” on the mug and ensure that\nthe movement of the text and the mug appear seamless and\nharmonious in the video.\nCurrent T2V methods are unsuitable for these settings, as\nthey typically focus on understanding the semantic-level in-\nformation from a given prompt rather than interpreting spe-\ncific words themselves. For instance, given a text input as\n“a person walking on the road,” current T2V models can in-\nterpret the scene and produce a corresponding video about a\nperson who walks on the road. However, these models fail\nto understand prompts at a more granular level. If the text\ninput is modified to “a person walking on the road, wearing\na T-shirt with the word ’Hello World’ printed on it,” the gen-\nerated results of current methods are far from satisfactory,\ndue to their inability to accurately interpret the generation of\nthe texts ’Hello World’ and incorporate its associated mo-\ntion information effectively.\nRecently, some preliminary efforts have been made\nin the field of visual text generation, specifically in the\nparadigm of Text-to-Image (T2I) generation [17, 27, 37].\nThese trials have shown promising results, but they are only\nlimited to the image domain. When extending this task to\nvideo scenarios, an intuitive approach is to use images gen-\nerated by these methods as input for cutting-edge image-to-\nvideo (I2V) methods. However, most current I2V methods\neither focus on learning motion patterns in simply natural\nscenes [2, 6, 29] or deliberately omit data that include visual\ntexts during dataset collection [2]. As a result, videos gen-\nerated by these methods fall into a dilemma generally called\ntext collapse, which means that as the number of frames in-\ncreases, the visualized text becomes increasingly blurry or\nloses its original structure (as demonstrated in Sec. 4 of this\npaper). Therefore, it is difficult to directly extend visual\ntext generation models from the image domain to the video\ndomain.\nBased on the above observations, we propose an effec-\ntive solution for visual text video generation, which can ef-\nfectively under texts for the description of videos and visualtexts that should be generated. Our method not only reflects\nthe semantics of the complete text, but also understands the\nfine-grain semantics of the input vocabulary, and effectively\naggregates the two in terms of content while maintaining a\ngood motion association (unable to visualize the movement\nof text and other content).\nTo achieve these goals, we propose a novel method\ncalled Text-Animator . Different from previous T2V meth-\nods, Text-Animator contains a text embedding injection\nmodule to enhance its precise understanding and genera-\ntion capacity for visual text. Besides, a unified controlling\nstrategy with camera and text position control is designed\nto improve the stability of the movement of visualized text\nand image content, thereby achieving unity and coordina-\ntion of the text movements. Specifically, for camera con-\ntrol, the control information is applied to the main body\nof the network by considering the features of the camera’s\nmotion trajectories. The position control aims at control-\nling the specific position and size of visual text generated\nin videos. Owing to the comprehensive controlling strategy']},
#  {'theme': ['Frame 1 Frame 5 Frame 10 Frame 15Figure 6. The example of large-area text movement, demonstrates\nthat our method does not cause damage to the text when moving\ntext over a large area.\nSpeed=4.0x\nSpeed=6.0xSpeed=1.0x\nframe 1 frame 5 frame 10\nFigure 7. The comparison of the same text and camera information\nat different speeds. The prompt is ‘A delicious and square cake\nwith the words ‘HAPPY”.\nexperiments, we have demonstrated that Text-Animator\noutperforms existing T2V and hybrid T2I/I2V methods in\nterms of video quality and fidelity of textual representation.\nOur contributions not only address current challenges\nbut also inspire further exploration and innovation in this\nrapidly evolving field of multimedia content generation.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477 , 2023. 3\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2, 6\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh. Video generation models as world simulators.\n2024. 3\n[4] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li,\nChanghua Meng, Huijia Zhu, Weiqiang Wang, et al. Diffute:Universal text editing diffusion model. Advances in Neural\nInformation Processing Systems , 36, 2024. 3\n[5] Ruoyu Guo Xiaoting Yin Kaitao Jiang Yongkun Du Yun-\ning Du Lingfeng Zhu Baohua Lai Xiaoguang Hu Dianhai\nYu Yanjun Ma Chenxia Li, Weiwei Liu. Pp-ocrv3: More at-\ntempts for the improvement of ultra lightweight ocr system.\narXiv preprint arXiv:2206.03001 , 2022. 3\n[6] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Animateanything: Fine-\ngrained open domain image animation with motion guid-\nance. arXiv e-prints , pages arXiv–2311, 2023. 2\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems , 35:16890–16902, 2022. 3\n[8] Gen-2, September 25, 2023. https://research.\nrunwayml.com/gen2 . 6, 7\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 3\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725 , 2023. 3, 5, 6, 7\n[11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling\ncamera control for text-to-video generation. arXiv preprint\narXiv:2404.02101 , 2024. 5\n[12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu,\nKe Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator:\nZero-shot identity-preserving human video generation. arXiv\npreprint arXiv:2404.15275 , 2024. 3\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221 ,\n2022. 3\n[14] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. ICLR , 2014. 3\n[15] Pika labs, 2023. https://www.pika.art/ . 6, 7\n[16] Lykon, 2023. https://huggingface.co/Lykon/\ndreamshaper-8 . 5\n[17] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,\nHaonan Lu, and Xiaodong Lin. Glyphdraw: Learning to\ndraw chinese characters in image synthesis models coher-\nently. arXiv preprint arXiv:2303.17870 , 2023. 2, 3',
#    'tent. Besides, in Fig. 5, we show one example of the\nLAION-subset dataset. Only our method can correctly dis-\nplay the visual characters (CHRISTMAS) and the number\nof bags (two).\nAt the same time, we also conducted experiments to ver-\nify the robustness of our method. In Fig. 6, we demonstrate\nthe robustness of our method for large movement in the text\nregion. The existing SOTA methods deformed the text area\nduring small movements (as shown in the example above),\nso the visualization results of these methods are not shown\nhere. The texts for these two examples are ‘A coffee mug\nwith the words ‘cafe’ on the office desk’ and ‘A bottom of\nmilk with the words ‘MILK’. The direction of movement is\nfrom right to left. We can see that the structure of our text\ncan still be maintained even with a large range of camera\nmovements. In Fig. 7, we demonstrate that under the same\ncamera information, we can control its movement speed by\nsampling the camera information of the interval frames. At\na speed of 4 or 6 times the original speed, our method is still\nable to maintain the structure of the text.\n4.5. Ablation Study\nIn this part, to illustrate the contributions of our method, we\nconduct ablation studies on LAION-subset. The quantita-\ntive comparisons are shown in Table 3.\nDual control : We conduct an ablation study to ana-\nlyze the effectiveness of the dual control design. Generally\nspeaking, it is feasible to use only position boxes for guid-\nance without using camera poses. Therefore, we designed\nthe ‘W/o camera control’ model, which removed the camera\nguidance module compared to the original model. In addi-\ntion, we removed the position block and only used camera\npose and glyph embedding, and named this model ‘W/o po-',
#    'Table 1. Quantitative comparison results on the LAION-subset dataset. The best results are shown in Bold and the second best are\nunderlined.\nMethod Parameters Sen. ACC ↑NED↑FID↓Prompt similarity ↑Frame similarity ↑\nAnytext [27] + AnimateLCM [29] 2726M 0.220 0.615 153.7 33.62 75.91\nAnytext [27] + I2VGen-XL [41] 2785M 0.267 0.582 184.9 30.18 79.74\nGlyphControl [37] + AnimateLCM [29] 2625M 0.139 0.303 182.3 34.00 76.03\nGlyphControl [37] + I2VGen-XL [41] 2684M 0.197 0.298 186.1 32.26 79.98\nAnimatediff-SDXL (Text Lora A) [10] 2927M 0.209 0.555 262.1 32.72 74.22\nAnimatediff-SDXL (Text Lora B) [10] 2927M 0.197 0.528 275.2 32.51 78.10\nOurs 1855M 0.779 0.802 180.6 33.78 92.66\nOpen -SORA\n AnimateDiff\n Ours SVD\n ModelScope\n Pika\nFigure 3. Qualitative comparison of Text-Animator and state-of-the-art T2V models or APIs in visual text generation. The prompt is ‘A\nred panda is holding a sign that says ‘HELLO”.\nTable 2. Quantitative comparison results on the LAION-subset\ndataset with some T2V methods.\nMethod Sen. ACC ↑NED↑\nOpen-SORA [43] – 0.081\nMorph Studio [26] – 0.255\nPika [15] 0.267 0.611\nGen-2 [8] 0.279 0.708\nOurs 0.779 0.802\nsition control’. In Table 3, we can see that on the xxx metric,\nthe performance of the ’W/o camera control’ model has de-\ncreased by 0.016 on NED compared to the original model,\nand the performance of the ’W/o position control’ model\nhas decreased by 0.027 on NED compared to the original\nmodel.\nPosition refinement and expansion size : We also con-\nduct experiments to analyze the effectiveness of our pro-\nposed refinement module. When the video position refine-Table 3. Ablation studies on the LAION-subset dataset.\nMethod Sen. ACC NED ↑FID↓\nW/o camera control 0.755 0.786 183.2\nW/o position control 0.732 0.775 185.7\nW/o position refinement 0.755 0.763 180.9\nExpansion size=0.9 0.779 0.804 181.9\nExpansion size=1.4 0.767 0.791 181.3\nFull model 0.779 0.802 180.6\nment is removed, we use the default position in the LAION\nsubset. And we denote the model as ‘w/o Position Refine-\nment’ in Table 3. We can see that the original position will\ndecrease the accuracy. Besides, we conduct experiments\nabout the proper expansion size. We tried two expansion\ncoefficients: 0.9 (smaller than 1.2) and 1.4 (larger than 1.2).\nIt can be observed that although the smaller expansion co-\nefficient improves the accuracy of the text in the video, it',
#    'laulampaul.github.io/text-animator.html .\n1. Introduction\nVideo generation has become an important cornerstone in\ncontent-based generation, and has huge potential value in\nvarious domains including e-commerce, advertising, the\nfilm industry, etc. For instance, in advertising scenarios,\nit is essential to display a manufacturer’s specific logo or\nslogan in the generated video in the form of text, while also\nseamlessly integrating text with the products featured in the\nvideo (e.g. a piece of clothing). However, in current video\ngeneration approaches, the visualization of text/words in the\ngenerated video remains a challenging yet unresolved is-\nsue. For example, in the first example of Fig. 1, we need\nto engrave the word ”CAFE” on the mug and ensure that\nthe movement of the text and the mug appear seamless and\nharmonious in the video.\nCurrent T2V methods are unsuitable for these settings, as\nthey typically focus on understanding the semantic-level in-\nformation from a given prompt rather than interpreting spe-\ncific words themselves. For instance, given a text input as\n“a person walking on the road,” current T2V models can in-\nterpret the scene and produce a corresponding video about a\nperson who walks on the road. However, these models fail\nto understand prompts at a more granular level. If the text\ninput is modified to “a person walking on the road, wearing\na T-shirt with the word ’Hello World’ printed on it,” the gen-\nerated results of current methods are far from satisfactory,\ndue to their inability to accurately interpret the generation of\nthe texts ’Hello World’ and incorporate its associated mo-\ntion information effectively.\nRecently, some preliminary efforts have been made\nin the field of visual text generation, specifically in the\nparadigm of Text-to-Image (T2I) generation [17, 27, 37].\nThese trials have shown promising results, but they are only\nlimited to the image domain. When extending this task to\nvideo scenarios, an intuitive approach is to use images gen-\nerated by these methods as input for cutting-edge image-to-\nvideo (I2V) methods. However, most current I2V methods\neither focus on learning motion patterns in simply natural\nscenes [2, 6, 29] or deliberately omit data that include visual\ntexts during dataset collection [2]. As a result, videos gen-\nerated by these methods fall into a dilemma generally called\ntext collapse, which means that as the number of frames in-\ncreases, the visualized text becomes increasingly blurry or\nloses its original structure (as demonstrated in Sec. 4 of this\npaper). Therefore, it is difficult to directly extend visual\ntext generation models from the image domain to the video\ndomain.\nBased on the above observations, we propose an effec-\ntive solution for visual text video generation, which can ef-\nfectively under texts for the description of videos and visualtexts that should be generated. Our method not only reflects\nthe semantics of the complete text, but also understands the\nfine-grain semantics of the input vocabulary, and effectively\naggregates the two in terms of content while maintaining a\ngood motion association (unable to visualize the movement\nof text and other content).\nTo achieve these goals, we propose a novel method\ncalled Text-Animator . Different from previous T2V meth-\nods, Text-Animator contains a text embedding injection\nmodule to enhance its precise understanding and genera-\ntion capacity for visual text. Besides, a unified controlling\nstrategy with camera and text position control is designed\nto improve the stability of the movement of visualized text\nand image content, thereby achieving unity and coordina-\ntion of the text movements. Specifically, for camera con-\ntrol, the control information is applied to the main body\nof the network by considering the features of the camera’s\nmotion trajectories. The position control aims at control-\nling the specific position and size of visual text generated\nin videos. Owing to the comprehensive controlling strategy']},
#  {'Text-Animator Controllable Visual Text Video Generation': ["2D U-Net Block\nTemporal Attention Block\nGlyph Position\nFuseText &\nPosition\nControlNetCamera\nControlNetCamera Pose Information\nVideo Position &\nGlyph Generation Video Position\nRefine Expansion Size\nFeatureCamera Contr ol Module (Sec. 3.3)Text Glyph and Position\nRefinement  Module (Sec. 3.4)Text Embedding\nInjection\nModule (Sec. 3.2)\nVAE\nDiffusion Pipeline\nA delicate square cake\nwith the word 'HAPPY'\nwritten on itPrompt\nT timesC\nC Cross AttentionFigure 2. Framework of Text-Animator. Given a pre-trained 3D-UNet, the camera ControlNet takes camera embedding as input and outputs\ncamera representations; the text and position ControlNet takes the combination feature zcas input and outputs position representations\nThese features are then integrated into the 2D Conv layers and temporal attention layers of 3D-UNet at their respective scales.\nous operations. Hence, the objective of training our encoder\nis shown below:\nL=Ez0,ϵ,ct,sp,sct[∥ϵ−ˆϵθ(zt, ct, Np(sp), Nc(sc), t)],\n(1)\nwhere ctis the embeddings of the corresponding text\nprompts, spis the set of the position maps and glyph\nmaps, and scis the set of camera pose information ( sc=\nK1, E1, K2, E2, ..., K n, En).\n3.2. Text Embedding Injection Module\nIn the generation of videos with visual text, the first con-\nsideration is how to effectively embed the visual features\nof the required text into the base model (the pre-trained\nUNet model). Inspired by previous methods of visualiz-\ning text in images [27, 37], we embed text conditions in the\nlatent space by combining the positions of text boxes and\nthe rendered glyphs. Text boxes indicate the positions in\nthe generated image where rendering should occur, while\nthe rendered glyphs utilize existing font style (i.e., ‘Arial\nUnicode’) to pre-initialize the style of the characters. In ad-\ndition, unlike image generation, video generation involves\nprocessing features across multiple frames. To leverage the\npre-trained feature extractor used in image generation, we\nextract features from each frame using a frame-wise feature\nextractor, and then concatenate these features before feed-\ning them into a pre-trained UNet model.From the top left of Fig. 2, we can see that the input to\nthe position and glyph control module is the position map\nP1, P2, ..., P nand glyph map G1, G2, ..., G ngenerated by\nthe module in Sec. 3.4. We extract features of glyphs and\npositions separately using glyph convolution blocks and po-\nsition convolution blocks, respectively. Then, we merge\nthese features using a fusion convolution block. Finally, af-\nter combining these features with the noisy input Zt, they\nare inputted into the text and position ControlNet. The text\nand position ControlNet output multi-scale feature maps\nFP\nk. Following the ControlNet [40], we fuse these features\ninto the intermediate block and upsampling blocks of the\nUNet network, where they are directly added to the corre-\nsponding features.\n3.3. Camera Control for Stable Text Generation\nAfter incorporating the text embedding injection module,\nour method is now capable of generating visual text videos\nwith text that moves following the scene. However, this\ntext movement can sometimes become disconnected from\nthe movement of objects within the video. For instance, in\nthe prompt ‘A sign that says ‘STOP’,’ the text part ”STOP”\nmight move to the right while the sign moves to the left.\nTo generate more stable videos, additional control modules\nneed to be designed. Therefore, we propose to use cam-\nera pose information to control the movement of text and\nensure consistency with the scene content. In this section,",
#    'OursAnytext image\nAnytext + \nSVD\nAnytext image\nAnytext + \nI2VGEN- XL\nAnytext + \nAnimateLCM\nAnytext image\n GlyphControl image\nGlyphControl + \nAnimateLCMVideo Video Video VideoFigure 4. Qualitative comparison of Text-Animator and the combination of state-of-the-art T2I visual text generation models (GpyphCon-\ntrol and Anytext) and I2V models (AnimateLCM [29], I2VGen-XL [41], and SVD). The prompt is ‘A girl wearing a blue T-shirt with the\nwords ‘BEAUTY’, slight smile, seaside background’.\nAnimateDiff + \nLora BPika\nOurs\nAnimateDiff\n+ Lora AAnytext + \ni2vgen- xl\nGlyphControl + \ni2vgen- xl \nGround Truth\n OpenSora\n Gen-2\n Morph Studio\nFigure 5. Qualitative comparison of Text-Animator and others on one example of the LAION-subset dataset. The prompt is ‘Two bags\nwith the word ’CHRISTMAS’ designed on it’. Other methods cannot generate the correct word (Please zoom to see the results).\nnegatively impacts the quality of the video generation. On\nthe other hand, the larger expansion coefficient causes some\ncharacters to appear repeatedly in the video, thereby reduc-\ning the accuracy of the text.\n5. Conclusion\nIn conclusion, this paper presents Text-Animator, an\ninnovative approach to address the challenge of integrating\ntextual elements effectively into generated videos withinthe visual text video generation domain. Text-Animator\nemphasizes not only semantic understanding of text but\nalso fine-grained textual semantics, ensuring that visualized\ntext is dynamically integrated into video content while\nmaintaining motion coherence. Our approach introduces\ndual control mechanisms—camera and position control-\nto synchronize text animation with video motion, thereby\nenhancing unity and coordination between textual elements\nand video scenes. Through extensive quantitative and visual',
#    "Text-Animator: Controllable Visual Text Video Generation\nLin Liu1Quande Liu2Shengju Qian2Yuan Zhou3\nWengang Zhou1Houqiang Li1Lingxi Xie4Qi Tian4\n1EEIS Department, University of Science and Technology of China\n2Tencent3Nanyang Technical University\n4Huawei Tech.\nA coffee mug on the office desk with the words ‘CAFE’\nA girl wearing a blue T shirt with ‘beauty’, seaside background\n A bottle of perfume with the words 'CLASSIC'A road sign on the road with the word ‘STOP’\nA raccoon stands in front of a street wall with the words ‘WELCOME’\nOn a street during the night, neon lights spell out ‘ 小吃街 ’\nFigure 1. Given a sentence with visualized words, our Text-Animator is able to produce a wide range of videos that not only show the\nsemantic information of given text prompts, but further align with the visualized words. Our method is a one stage method without further\ntuning.\nAbstract\nVideo generation is a challenging yet pivotal task in var-\nious industries, such as gaming, e-commerce, and adver-\ntising. One significant unresolved aspect within T2V is the\neffective visualization of text within generated videos. De-\nspite the progress achieved in Text-to-Video (T2V) genera-\ntion, current methods still cannot effectively visualize texts\nin videos directly, as they mainly focus on summarizing se-\nmantic scene information, understanding, and depicting ac-\ntions. While recent advances in image-level visual text gen-\neration show promise, transitioning these techniques into\nthe video domain faces problems, notably in preserving tex-tual fidelity and motion coherence. In this paper, we pro-\npose an innovative approach termed Text-Animator for vi-\nsual text video generation. Text-Animator contains a text\nembedding injection module to precisely depict the struc-\ntures of visual text in generated videos. Besides, we develop\na camera control module and a text refinement module to\nimprove the stability of generated visual text by control-\nling the camera movement as well as the motion of visu-\nalized text. Quantitative and qualitative experimental re-\nsults demonstrate the superiority of our approach to the ac-\ncuracy of generated visual text over state-of-the-art video\ngeneration methods. The project page can be found inarXiv:2406.17777v1  [cs.CV]  25 Jun 2024",
#    'laulampaul.github.io/text-animator.html .\n1. Introduction\nVideo generation has become an important cornerstone in\ncontent-based generation, and has huge potential value in\nvarious domains including e-commerce, advertising, the\nfilm industry, etc. For instance, in advertising scenarios,\nit is essential to display a manufacturer’s specific logo or\nslogan in the generated video in the form of text, while also\nseamlessly integrating text with the products featured in the\nvideo (e.g. a piece of clothing). However, in current video\ngeneration approaches, the visualization of text/words in the\ngenerated video remains a challenging yet unresolved is-\nsue. For example, in the first example of Fig. 1, we need\nto engrave the word ”CAFE” on the mug and ensure that\nthe movement of the text and the mug appear seamless and\nharmonious in the video.\nCurrent T2V methods are unsuitable for these settings, as\nthey typically focus on understanding the semantic-level in-\nformation from a given prompt rather than interpreting spe-\ncific words themselves. For instance, given a text input as\n“a person walking on the road,” current T2V models can in-\nterpret the scene and produce a corresponding video about a\nperson who walks on the road. However, these models fail\nto understand prompts at a more granular level. If the text\ninput is modified to “a person walking on the road, wearing\na T-shirt with the word ’Hello World’ printed on it,” the gen-\nerated results of current methods are far from satisfactory,\ndue to their inability to accurately interpret the generation of\nthe texts ’Hello World’ and incorporate its associated mo-\ntion information effectively.\nRecently, some preliminary efforts have been made\nin the field of visual text generation, specifically in the\nparadigm of Text-to-Image (T2I) generation [17, 27, 37].\nThese trials have shown promising results, but they are only\nlimited to the image domain. When extending this task to\nvideo scenarios, an intuitive approach is to use images gen-\nerated by these methods as input for cutting-edge image-to-\nvideo (I2V) methods. However, most current I2V methods\neither focus on learning motion patterns in simply natural\nscenes [2, 6, 29] or deliberately omit data that include visual\ntexts during dataset collection [2]. As a result, videos gen-\nerated by these methods fall into a dilemma generally called\ntext collapse, which means that as the number of frames in-\ncreases, the visualized text becomes increasingly blurry or\nloses its original structure (as demonstrated in Sec. 4 of this\npaper). Therefore, it is difficult to directly extend visual\ntext generation models from the image domain to the video\ndomain.\nBased on the above observations, we propose an effec-\ntive solution for visual text video generation, which can ef-\nfectively under texts for the description of videos and visualtexts that should be generated. Our method not only reflects\nthe semantics of the complete text, but also understands the\nfine-grain semantics of the input vocabulary, and effectively\naggregates the two in terms of content while maintaining a\ngood motion association (unable to visualize the movement\nof text and other content).\nTo achieve these goals, we propose a novel method\ncalled Text-Animator . Different from previous T2V meth-\nods, Text-Animator contains a text embedding injection\nmodule to enhance its precise understanding and genera-\ntion capacity for visual text. Besides, a unified controlling\nstrategy with camera and text position control is designed\nto improve the stability of the movement of visualized text\nand image content, thereby achieving unity and coordina-\ntion of the text movements. Specifically, for camera con-\ntrol, the control information is applied to the main body\nof the network by considering the features of the camera’s\nmotion trajectories. The position control aims at control-\nling the specific position and size of visual text generated\nin videos. Owing to the comprehensive controlling strategy']},
#  {'achievements': ['Frame 1 Frame 5 Frame 10 Frame 15Figure 6. The example of large-area text movement, demonstrates\nthat our method does not cause damage to the text when moving\ntext over a large area.\nSpeed=4.0x\nSpeed=6.0xSpeed=1.0x\nframe 1 frame 5 frame 10\nFigure 7. The comparison of the same text and camera information\nat different speeds. The prompt is ‘A delicious and square cake\nwith the words ‘HAPPY”.\nexperiments, we have demonstrated that Text-Animator\noutperforms existing T2V and hybrid T2I/I2V methods in\nterms of video quality and fidelity of textual representation.\nOur contributions not only address current challenges\nbut also inspire further exploration and innovation in this\nrapidly evolving field of multimedia content generation.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477 , 2023. 3\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2, 6\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh. Video generation models as world simulators.\n2024. 3\n[4] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li,\nChanghua Meng, Huijia Zhu, Weiqiang Wang, et al. Diffute:Universal text editing diffusion model. Advances in Neural\nInformation Processing Systems , 36, 2024. 3\n[5] Ruoyu Guo Xiaoting Yin Kaitao Jiang Yongkun Du Yun-\ning Du Lingfeng Zhu Baohua Lai Xiaoguang Hu Dianhai\nYu Yanjun Ma Chenxia Li, Weiwei Liu. Pp-ocrv3: More at-\ntempts for the improvement of ultra lightweight ocr system.\narXiv preprint arXiv:2206.03001 , 2022. 3\n[6] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Animateanything: Fine-\ngrained open domain image animation with motion guid-\nance. arXiv e-prints , pages arXiv–2311, 2023. 2\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems , 35:16890–16902, 2022. 3\n[8] Gen-2, September 25, 2023. https://research.\nrunwayml.com/gen2 . 6, 7\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 3\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725 , 2023. 3, 5, 6, 7\n[11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling\ncamera control for text-to-video generation. arXiv preprint\narXiv:2404.02101 , 2024. 5\n[12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu,\nKe Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator:\nZero-shot identity-preserving human video generation. arXiv\npreprint arXiv:2404.15275 , 2024. 3\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221 ,\n2022. 3\n[14] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. ICLR , 2014. 3\n[15] Pika labs, 2023. https://www.pika.art/ . 6, 7\n[16] Lykon, 2023. https://huggingface.co/Lykon/\ndreamshaper-8 . 5\n[17] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,\nHaonan Lu, and Xiaodong Lin. Glyphdraw: Learning to\ndraw chinese characters in image synthesis models coher-\nently. arXiv preprint arXiv:2303.17870 , 2023. 2, 3',
#    'tent. Besides, in Fig. 5, we show one example of the\nLAION-subset dataset. Only our method can correctly dis-\nplay the visual characters (CHRISTMAS) and the number\nof bags (two).\nAt the same time, we also conducted experiments to ver-\nify the robustness of our method. In Fig. 6, we demonstrate\nthe robustness of our method for large movement in the text\nregion. The existing SOTA methods deformed the text area\nduring small movements (as shown in the example above),\nso the visualization results of these methods are not shown\nhere. The texts for these two examples are ‘A coffee mug\nwith the words ‘cafe’ on the office desk’ and ‘A bottom of\nmilk with the words ‘MILK’. The direction of movement is\nfrom right to left. We can see that the structure of our text\ncan still be maintained even with a large range of camera\nmovements. In Fig. 7, we demonstrate that under the same\ncamera information, we can control its movement speed by\nsampling the camera information of the interval frames. At\na speed of 4 or 6 times the original speed, our method is still\nable to maintain the structure of the text.\n4.5. Ablation Study\nIn this part, to illustrate the contributions of our method, we\nconduct ablation studies on LAION-subset. The quantita-\ntive comparisons are shown in Table 3.\nDual control : We conduct an ablation study to ana-\nlyze the effectiveness of the dual control design. Generally\nspeaking, it is feasible to use only position boxes for guid-\nance without using camera poses. Therefore, we designed\nthe ‘W/o camera control’ model, which removed the camera\nguidance module compared to the original model. In addi-\ntion, we removed the position block and only used camera\npose and glyph embedding, and named this model ‘W/o po-',
#    'motion trajectories. The position control aims at control-\nling the specific position and size of visual text generated\nin videos. Owing to the comprehensive controlling strategy\nover the developed text embedding injection module, Text-\nAnimator shows a superior capacity to generate stable and\naccurate visual text content in videos.\nIn summary, the contributions of this paper can be con-\ncluded below:\n• We propose Text-Animator, a novel approach that can\ngenerate visual text in videos and maintain the structure\nconsistency of generated visual texts. To our knowledge,\nthis is the first attempt at the visual text video generation\nproblem.\n• We develop a text embedding injection module for Text-\nAnimator that can accurately depict the structural infor-\nmation of visual text. Besides, we also propose a camera\ncontrol and text refinement module to accurately control\nthe camera movement and the motion of the generated vi-\nsual text, to improve the generation stability.\n• Extensive experiments demonstrate that Text-Animator\noutperforms current text-to-video and image-to-video\ngeneration methods by a large margin on the accuracy of\ngenerated visual text.\n2. Related Work\n2.1. Visual Text Generation\nThe goal of visual text generation is to integrate user-\nspecified texts into images or videos and produce well-\nformed and readable visual text, therefore effectively ensur-\ning that the texts fit well with the corresponding image con-\ntent. Current research mainly focuses on how to design an\neffective text encoder and considers the better guidance of\ntext-conditioned controlling information. For text encoder,\nas large language models develop [20, 21], it is a promising\nidea to directly use these models to encode text. However,',
#    'Table 1. Quantitative comparison results on the LAION-subset dataset. The best results are shown in Bold and the second best are\nunderlined.\nMethod Parameters Sen. ACC ↑NED↑FID↓Prompt similarity ↑Frame similarity ↑\nAnytext [27] + AnimateLCM [29] 2726M 0.220 0.615 153.7 33.62 75.91\nAnytext [27] + I2VGen-XL [41] 2785M 0.267 0.582 184.9 30.18 79.74\nGlyphControl [37] + AnimateLCM [29] 2625M 0.139 0.303 182.3 34.00 76.03\nGlyphControl [37] + I2VGen-XL [41] 2684M 0.197 0.298 186.1 32.26 79.98\nAnimatediff-SDXL (Text Lora A) [10] 2927M 0.209 0.555 262.1 32.72 74.22\nAnimatediff-SDXL (Text Lora B) [10] 2927M 0.197 0.528 275.2 32.51 78.10\nOurs 1855M 0.779 0.802 180.6 33.78 92.66\nOpen -SORA\n AnimateDiff\n Ours SVD\n ModelScope\n Pika\nFigure 3. Qualitative comparison of Text-Animator and state-of-the-art T2V models or APIs in visual text generation. The prompt is ‘A\nred panda is holding a sign that says ‘HELLO”.\nTable 2. Quantitative comparison results on the LAION-subset\ndataset with some T2V methods.\nMethod Sen. ACC ↑NED↑\nOpen-SORA [43] – 0.081\nMorph Studio [26] – 0.255\nPika [15] 0.267 0.611\nGen-2 [8] 0.279 0.708\nOurs 0.779 0.802\nsition control’. In Table 3, we can see that on the xxx metric,\nthe performance of the ’W/o camera control’ model has de-\ncreased by 0.016 on NED compared to the original model,\nand the performance of the ’W/o position control’ model\nhas decreased by 0.027 on NED compared to the original\nmodel.\nPosition refinement and expansion size : We also con-\nduct experiments to analyze the effectiveness of our pro-\nposed refinement module. When the video position refine-Table 3. Ablation studies on the LAION-subset dataset.\nMethod Sen. ACC NED ↑FID↓\nW/o camera control 0.755 0.786 183.2\nW/o position control 0.732 0.775 185.7\nW/o position refinement 0.755 0.763 180.9\nExpansion size=0.9 0.779 0.804 181.9\nExpansion size=1.4 0.767 0.791 181.3\nFull model 0.779 0.802 180.6\nment is removed, we use the default position in the LAION\nsubset. And we denote the model as ‘w/o Position Refine-\nment’ in Table 3. We can see that the original position will\ndecrease the accuracy. Besides, we conduct experiments\nabout the proper expansion size. We tried two expansion\ncoefficients: 0.9 (smaller than 1.2) and 1.4 (larger than 1.2).\nIt can be observed that although the smaller expansion co-\nefficient improves the accuracy of the text in the video, it']},
#  {'authors': ['Frame 1 Frame 5 Frame 10 Frame 15Figure 6. The example of large-area text movement, demonstrates\nthat our method does not cause damage to the text when moving\ntext over a large area.\nSpeed=4.0x\nSpeed=6.0xSpeed=1.0x\nframe 1 frame 5 frame 10\nFigure 7. The comparison of the same text and camera information\nat different speeds. The prompt is ‘A delicious and square cake\nwith the words ‘HAPPY”.\nexperiments, we have demonstrated that Text-Animator\noutperforms existing T2V and hybrid T2I/I2V methods in\nterms of video quality and fidelity of textual representation.\nOur contributions not only address current challenges\nbut also inspire further exploration and innovation in this\nrapidly evolving field of multimedia content generation.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477 , 2023. 3\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2, 6\n[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\nRamesh. Video generation models as world simulators.\n2024. 3\n[4] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li,\nChanghua Meng, Huijia Zhu, Weiqiang Wang, et al. Diffute:Universal text editing diffusion model. Advances in Neural\nInformation Processing Systems , 36, 2024. 3\n[5] Ruoyu Guo Xiaoting Yin Kaitao Jiang Yongkun Du Yun-\ning Du Lingfeng Zhu Baohua Lai Xiaoguang Hu Dianhai\nYu Yanjun Ma Chenxia Li, Weiwei Liu. Pp-ocrv3: More at-\ntempts for the improvement of ultra lightweight ocr system.\narXiv preprint arXiv:2206.03001 , 2022. 3\n[6] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Animateanything: Fine-\ngrained open domain image animation with motion guid-\nance. arXiv e-prints , pages arXiv–2311, 2023. 2\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems , 35:16890–16902, 2022. 3\n[8] Gen-2, September 25, 2023. https://research.\nrunwayml.com/gen2 . 6, 7\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems , 27, 2014. 3\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725 , 2023. 3, 5, 6, 7\n[11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo\nDai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling\ncamera control for text-to-video generation. arXiv preprint\narXiv:2404.02101 , 2024. 5\n[12] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu,\nKe Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator:\nZero-shot identity-preserving human video generation. arXiv\npreprint arXiv:2404.15275 , 2024. 3\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. arXiv preprint arXiv:2211.13221 ,\n2022. 3\n[14] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. ICLR , 2014. 3\n[15] Pika labs, 2023. https://www.pika.art/ . 6, 7\n[16] Lykon, 2023. https://huggingface.co/Lykon/\ndreamshaper-8 . 5\n[17] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,\nHaonan Lu, and Xiaodong Lin. Glyphdraw: Learning to\ndraw chinese characters in image synthesis models coher-\nently. arXiv preprint arXiv:2303.17870 , 2023. 2, 3',
#    'this roadmap inevitably results in overlooking the character\nfeatures of texts. Recently, some works have optimized text\nencoders for character features. GlyphDraw [17] fine-tuned\nthe text encoder for Chinese images for glyph embeddings.\nChen et al. [4] trained a glyph-extracted image encoder for\nimage editing. AnyText [27] utilizes pretrained recognition\nmodel, PP-OCRv3 [5] for encoding text.\nTo generate characters more accurately, control infor-\nmation of the text is required as additional input. Glyph-\nDraw [17] uses explicit glyph images as conditions to ren-\nder characters. GlyphControl [37] and AnyText [27] em-\nbed text conditions in the latent space by the combination\nof the positions of text boxes and the rendered glyphs. dif-\nferent from [17, 27, 37], Yang et al. [37] use character-level\nsegmentation masks as conditioned controlling information,\nallowing for finer granularity control. To our knowledge,\ncurrent methods mainly focus on addressing the visual text\ngeneration problem in the text-to-image domain, which can-\nnot be utilized to tackle text-to-video visual text genera-\ntion.In this paper, we first explore the visual text generation\ntask in the video domain.\n2.2. Video Generation\nSora [3], a recent famous video generation model has at-\ntracted much attention from both the community of both\nindustry and academia. Before the emergence of diffusion-\nbased models, lots of effort in this field have been paid on\nmethods based on GANs [9] or VQV AE [19, 28]. Among\nthese methods, the pre-trained Text-to-Image (T2I) model\nCogView2 [7] is utilized in CogVideo [7] as the back-\nbone, to enable generating long sequence videos in an\nauto-regressive way. Based on autoregressive Transform-\ners, NUWA [34] combines three tasks, which are T2I, T2V ,\nand video prediction.\nCurrently, diffusion models have become the mainstream\nmethod in video generation. Make-A-Video [24] proposes\nto learn visual-textual correlations and thus capture video\nmotion from unsupervised data. Some methods [1, 13, 30,\n44] design effective temporal modules to reduce computa-\ntional complexity and model temporal relationships effec-\ntively. Multi-stage approaches [23, 32, 39] design models to\nbe used in stages for achieving high-definition video gener-\nation. These methods highlight the versatility and efficacy\nof diffusion models in advancing video generation capabil-\nity.\n2.3. Controllable video generation\nIn addition to conventional T2V models, some methods fo-\ncus on making video generation controllable. In these meth-\nods, [12, 35, 36, 42] turn to refer to specific video templates\nfor controlling motion. However, despite the effectiveness\nof these methods in motion controlling, they typically re-\nquire training new models on each template or templateset, limiting their capability in controllable video genera-\ntion. Besides, VideoComposer [31] proposes to use motion\nvectors to control the video motion. MotionCtrl [33] de-\nsigns two control modules for camera motion and object\nmotion control. Drag-NUWA [38] uses trajectories and text\nprompts in a joint way for video generation conditioned on\nan initial image. Different from these approaches, a dual\ncontrol visual text generation model is utilized in our Text-\nAnimator, where camera pose information and position tra-\njectories can effectively control the motion of videos and\nmake the generation process more stable.\n3. Method\nIn this section, we first introduce the pipeline of our Text-\nAnimator in Sec. 3.1. Then, the details of the key compo-\nnents are introduced in Sec. 3.2, Sec. 3.3, and Sec. 3.4\nrespectively.\n3.1. Text-conditioned Video Generation Pipeline\nFirstly, we introduce the overall framework of our net-\nwork, as shown in Fig. 2. Our method consists of four\nparts that are Text Embedding Injection Module, Cam-\nera Control Module, Text Glyph and Position Refinement\nModule, and 3D-UNet Module. Given the integrated texts\nTin, position map P1,ori, and Camera Pose Information',
#    'motion trajectories. The position control aims at control-\nling the specific position and size of visual text generated\nin videos. Owing to the comprehensive controlling strategy\nover the developed text embedding injection module, Text-\nAnimator shows a superior capacity to generate stable and\naccurate visual text content in videos.\nIn summary, the contributions of this paper can be con-\ncluded below:\n• We propose Text-Animator, a novel approach that can\ngenerate visual text in videos and maintain the structure\nconsistency of generated visual texts. To our knowledge,\nthis is the first attempt at the visual text video generation\nproblem.\n• We develop a text embedding injection module for Text-\nAnimator that can accurately depict the structural infor-\nmation of visual text. Besides, we also propose a camera\ncontrol and text refinement module to accurately control\nthe camera movement and the motion of the generated vi-\nsual text, to improve the generation stability.\n• Extensive experiments demonstrate that Text-Animator\noutperforms current text-to-video and image-to-video\ngeneration methods by a large margin on the accuracy of\ngenerated visual text.\n2. Related Work\n2.1. Visual Text Generation\nThe goal of visual text generation is to integrate user-\nspecified texts into images or videos and produce well-\nformed and readable visual text, therefore effectively ensur-\ning that the texts fit well with the corresponding image con-\ntent. Current research mainly focuses on how to design an\neffective text encoder and considers the better guidance of\ntext-conditioned controlling information. For text encoder,\nas large language models develop [20, 21], it is a promising\nidea to directly use these models to encode text. However,',
#    'transferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 2\n[21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of machine learning\nresearch , 21(140):1–67, 2020. 2\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2022. 5\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in neural information\nprocessing systems , 35:36479–36494, 2022. 3\n[24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. 2022. 3\n[25] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh\nTenenbaum, and Fredo Durand. Light field networks: Neu-\nral scene representations with single-evaluation rendering.\nAdvances in Neural Information Processing Systems , 34:\n19313–19325, 2021. 5\n[26] Morph studio, 2023. https://app.morphstudio.\ncom/ . 6, 7\n[27] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng,\nand Xuansong Xie. Anytext: Multilingual visual text gen-\neration and editing. arXiv preprint arXiv:2311.03054 , 2023.\n2, 3, 4, 5, 6, 7\n[28] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems , 30, 2017. 3\n[29] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian,\nGuanglu Song, Yu Liu, and Hongsheng Li. Animatelcm:\nAccelerating the animation of personalized diffusion mod-\nels and adapters with decoupled consistency learning. arXiv\npreprint arXiv:2402.00769 , 2024. 2, 6, 7, 8\n[30] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571 , 2023. 3,\n6\n[31] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Videocomposer: Compositional video synthesis\nwith motion controllability. Advances in Neural Information\nProcessing Systems , 36, 2024. 3\n[32] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103 , 2023. 3[33] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen,\nMenghan Xia, Ping Luo, and Ying Shan. Motionctrl: A\nunified and flexible motion controller for video generation.\narXiv preprint arXiv:2312.03641 , 2023. 3\n[34] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N ¨uwa: Visual synthesis pre-\ntraining for neural visual world creation. In European con-\nference on computer vision , pages 720–736. Springer, 2022.\n3\n[35] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 7623–7633, 2023. 3\n[36] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi\nLi, and Xiangyu Zhang. Lamp: Learn a motion pat-\ntern for few-shot-based video generation. arXiv preprint\narXiv:2310.10769 , 2023. 3\n[37] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang,\nHaisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph']}]
# llm_result = 'The Three-Body Problem" is a science fiction novel by Liu Cixin that explores the interaction between humanity and an alien civilization from the star system Trisolaris. The story begins during China\'s Cultural Revolution and follows several characters, including astrophysicist Ye Wenjie, who becomes disillusioned with humanity and sends a signal to the Trisolarans, inviting them to Earth. The Trisolarans, whose planet suffers from extreme and unpredictable climate changes due to its three suns, see Earth as a potential new home. The novel delves into themes of scientific discovery, the potential dangers of first contact with an alien civilization, and the moral and ethical dilemmas faced by humanity. Key scientific concepts include the three-body problem in orbital mechanics, which describes the complex gravitational interactions between three celestial bodies, and the use of advanced technology by the Trisolarans to disrupt human scientific progress.'
# quality_enhancer = QualityEnhancerModule()
# result = quality_enhancer.forward(question=question,llm_data='', rag_data=json.dumps(rag_data))
# print(result)
#


