{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:29:41.887501Z",
     "start_time": "2024-05-21T06:29:17.628642Z"
    }
   },
   "source": [
    "#Dependencies\n",
    "!pip install dspy-ai[chromadb] -Uqq\n",
    "!pip install termcolor -Uqq\n",
    "!pip install sqlalchemy -Uqq\n",
    "!pip install lxml -Uqq\n",
    "!pip install xlrd -Uqq\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIM OF THE TUTORIAL\n",
    "\n",
    "* Build an end-to-end Text-to-SQL pipeline inspired from this [video](https://www.youtube.com/watch?v=L1o1VPVfbb0&pp=ygUYYWR2YW5jZWQgUkFHIGxsYW1hIGluZGV4) from Llama Index. In Llama Index, they used llama index Query Pipeline to build a Text-to-SQL pipeline. Here, we will build a Text-to-SQL pipeline based on our own dataset and from scratch. We will go from the dataset scraping, to building SQLlite database and using DSPy signatures to implementa a text-to-SQL pipeline (从 Llama Index 的 [视频](https:www.youtube.comwatch?v=L1o1VPVfbb0&pp=ygUYYWR2YW5jZWQgUkFHIGxsYW1hIGluZGV4) 中获得灵感，构建端到端的文本到 SQL 管道。在 Llama Index 中，他们使用了 llama index Query Pipeline 来构建文本到 SQL 管道。在这里，我们将基于自己的数据集，从零开始构建文本到 SQL 管道。我们将从数据集搜刮开始，到构建 SQLlite 数据库并使用 DSPy 签名来实现文本到 SQL 管道)\n",
    "\n",
    "## ABOUT THE DATASET\n",
    "* You can find the dataset [here](https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html). The dataset has different industry based different financial metrics like WACC, tax rates, EBITDA, etc. There are multiple regions data `['US', 'Europe', 'Japan', 'AUS_NZ_CANADA', 'Emerging', 'China', 'India', 'Global']` where we have multiple tables for each region. There are nearly 250 tables with multiple columns in each table. We will build a text-to-SQL pipeline based on our own dataset and from scratch, starting from embedding tables schema and rows using ChromDB vector database. (该数据集具有基于不同行业的不同财务指标，如 WACC、税率、EBITDA 等。有多个区域数据“[美国'、'欧洲'、'日本'、'AUS_NZ_CANADA'、'新兴'、'中国'、'印度'、'全球']'，每个区域都有多个表。有近 250 个表，每个表中有多列。我们将基于自己的数据集从头开始构建一个文本到 SQL 管道，从使用 ChromDB 矢量数据库嵌入表架构和行开始。)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:17.837383Z",
     "start_time": "2024-05-21T08:21:14.406954Z"
    }
   },
   "source": [
    "from experiment_project.utils.files.read import read_yaml\n",
    "from experiment_project.utils.initial.util import init_sys_env\n",
    "#imports\n",
    "from bs4 import BeautifulSoup \n",
    "import urllib.request\n",
    "import ssl\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "from sqlalchemy import inspect\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text \n",
    "import dspy\n",
    "from typing import List\n",
    "from termcolor import colored\n",
    "import chromadb\n",
    "init_sys_env()\n",
    "secret_env_file = '/mnt/d/project/dy/extra/autogen/env_secret_config.yaml'\n",
    "\n",
    "api_configs = read_yaml(secret_env_file)\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:17.845037Z",
     "start_time": "2024-05-21T08:21:17.839251Z"
    }
   },
   "source": [
    "from dsp.modules.cache_utils import cache_turn_on\n",
    "\n",
    "cache_turn_on"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## SCRAPING THE LINKS OF THE EXCEL FILES FROM THE WEBSITE(从网站上抓取 excel 文件的链接)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:34:03.801273Z",
     "start_time": "2024-05-21T06:33:58.238790Z"
    }
   },
   "source": [
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "html_link = \"https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html\"\n",
    "\n",
    "with urllib.request.urlopen(html_link) as url:\n",
    "    s = url.read()\n",
    "    # I'm guessing this would output the html source code ?\n",
    "    soup = BeautifulSoup(s,\"lxml\")\n",
    "\n",
    "html_table = soup.find_all(\"table\")\n",
    "req_table = html_table[1]\n",
    "hrefs_list = req_table.find_all('a')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:34:03.807668Z",
     "start_time": "2024-05-21T06:34:03.803384Z"
    }
   },
   "source": [
    "req_href = {\"US\":[],\"Europe\":[],\"Japan\":[],\"AUS_NZ_CANADA\":[],\"Emerging\":[],\"China\":[],\"India\":[],\"Global\":[]}\n",
    "\n",
    "for i in hrefs_list:\n",
    "    name = i.get_text().strip()\n",
    "    try:\n",
    "        href_attr = i['href']\n",
    "        # Only get the excel files\n",
    "        if href_attr.endswith('.xls'):\n",
    "            if \"US\" in name:\n",
    "                req_href[\"US\"].append(href_attr)\n",
    "            elif \"Europe\" in name:\n",
    "                req_href[\"Europe\"].append(href_attr)\n",
    "            elif \"Japan\" in name:\n",
    "                req_href[\"Japan\"].append(href_attr)\n",
    "            elif \"Aus\" in name:\n",
    "                req_href['AUS_NZ_CANADA'].append(href_attr)\n",
    "            elif \"Emerging\" in name:\n",
    "                req_href['Emerging'].append(href_attr)\n",
    "            elif \"China\" in name:\n",
    "                req_href['China'].append(href_attr)\n",
    "            elif \"India\" in name:\n",
    "                req_href['India'].append(href_attr)\n",
    "            elif \"Global\" in name: \n",
    "                req_href['Global'].append(href_attr)\n",
    "    except:\n",
    "        pass"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:44:15.396367Z",
     "start_time": "2024-05-21T06:43:49.654444Z"
    }
   },
   "source": [
    "# #Download the excel files from the website and store it in a folder named DATA\n",
    "# ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "# \n",
    "# import os\n",
    "# os.makedirs(\"DATA\",exist_ok=True)\n",
    "# for country,excel_files in req_href.items():\n",
    "#     country_path = os.path.join(\"DATA\",country) \n",
    "#     os.makedirs(country_path,exist_ok=True)\n",
    "#     for file in excel_files:\n",
    "#         file_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "#         full_file_name = os.path.join(country_path,f\"{file_name}.xls\")\n",
    "#         resp = requests.get(file,verify=False)\n",
    "#         output = open(full_file_name, 'wb')\n",
    "#         output.write(resp.content)\n",
    "#         output.close()"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:46:12.626126Z",
     "start_time": "2024-05-21T06:46:12.621346Z"
    }
   },
   "cell_type": "code",
   "source": "req_href",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:54:08.535500Z",
     "start_time": "2024-05-21T06:46:42.854986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import ssl\n",
    "import requests\n",
    "from requests.exceptions import ChunkedEncodingError\n",
    "from time import sleep\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# 创建数据存储文件夹\n",
    "os.makedirs(\"DATA\", exist_ok=True)\n",
    "# 创建会话\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=1)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# 下载文件\n",
    "for country, excel_files in req_href.items():\n",
    "    country_path = os.path.join(\"DATA\", country)\n",
    "    os.makedirs(country_path, exist_ok=True)\n",
    "    for file in excel_files:\n",
    "        file_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        full_file_name = os.path.join(country_path, f\"{file_name}.xls\")\n",
    "        success = False\n",
    "        attempts = 0\n",
    "        while not success and attempts < 5:\n",
    "            try:\n",
    "                resp = session.get(file, verify=False, stream=True)\n",
    "                resp.raise_for_status()  # 检查请求是否成功\n",
    "                with open(full_file_name, 'wb') as output:\n",
    "                    for chunk in resp.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            output.write(chunk)\n",
    "                success = True\n",
    "            except ChunkedEncodingError as e:\n",
    "                attempts += 1\n",
    "                print(f\"ChunkedEncodingError: {e}. Retrying {attempts}/5...\")\n",
    "                sleep(2)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"RequestException: {e}\")\n",
    "                break\n"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T06:55:22.547072Z",
     "start_time": "2024-05-21T06:55:22.529447Z"
    }
   },
   "source": [
    "# Sanity check\n",
    "for country in os.listdir(\"DATA\"):\n",
    "    dir_len = len(os.listdir(os.path.join(\"DATA\",country)))\n",
    "    country_len = len(req_href[country])\n",
    "    print(f'FOR {country} WE HAVE DIRECTORY LEN = {dir_len} and ACTUAL LEN = {country_len}')"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:00:54.906528Z",
     "start_time": "2024-05-21T07:00:54.890992Z"
    }
   },
   "source": "sample_excel = pd.ExcelFile(\"DATA/US/capex.xls\")",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:01:00.172299Z",
     "start_time": "2024-05-21T07:01:00.158670Z"
    }
   },
   "source": [
    "sn = 'Variables & FAQ'\n",
    "sample_excel.parse(sn).head(10)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:01:05.171492Z",
     "start_time": "2024-05-21T07:01:05.161212Z"
    }
   },
   "source": [
    "sample_excel.parse(sample_excel.sheet_names[1]).head(10)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the dataset above, there are two sheets. The first sheet is the variables and summary, and the second sheet is the table with the data. \n",
    "* We will clean the first sheet to get the table name and summary\n",
    "* 在上面的数据集中，有两张表。 第一张表是变量和摘要，第二张表是包含数据的表格。\n",
    "* 我们将清理第一张表以获取表名称和摘要"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:03:06.049348Z",
     "start_time": "2024-05-21T07:03:06.008133Z"
    }
   },
   "source": [
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "# pd.set_option('display.max_rows', 50)\n",
    "# \n",
    "# def sanitize_column_name(col_name):\n",
    "#     # Remove special characters and replace spaces with underscores\n",
    "#     return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "# \n",
    "# dir = \"DATA\"\n",
    "# processed_dir = \"Processed Data\"\n",
    "# all_infos_dict = []\n",
    "# os.makedirs(processed_dir,exist_ok=True)\n",
    "# for country in os.listdir(dir):\n",
    "#     print(country)\n",
    "#     file_name = os.path.join(dir,country)\n",
    "#     os.makedirs(os.path.join(processed_dir,country),exist_ok=True)\n",
    "#     os.makedirs(file_name,exist_ok=True)\n",
    "#     for excel_file in tqdm(os.listdir(file_name)):\n",
    "#         full_file_name = os.path.join(file_name,excel_file)\n",
    "#         xls = pd.ExcelFile(full_file_name)\n",
    "#         sns = xls.sheet_names\n",
    "#         for sheet_name in sns:\n",
    "#             if \"Var\" in sheet_name or \"var\" in sheet_name:\n",
    "#                 info_df = xls.parse(sheet_name)\n",
    "#                 info_df.dropna(how=\"all\",inplace=True)\n",
    "#                 info_dict = {}\n",
    "#                 for cols in info_df.columns:\n",
    "#                     if \"End\" not in cols and 'Unnamed' not in cols:\n",
    "#                         info_dict['Summary'] = cols\n",
    "#                 info_dict['Vars'] = info_df.values[1:].tolist()\n",
    "#                 all_infos_dict.append(info_dict)\n",
    "#             elif \"Industry\" in sheet_name or \"industry\" in sheet_name:\n",
    "#                 data_df = xls.parse(sheet_name)\n",
    "#         try:\n",
    "#             data_df.dropna(axis=1,thresh=5,inplace=True)\n",
    "#             data_df.dropna(inplace=True)\n",
    "#             new_header = data_df.iloc[0] #grab the first row for the header\n",
    "#         except:\n",
    "#             print(full_file_name)\n",
    "#             print(data_df)\n",
    "#         data_df = data_df[1:] #take the data less the header row\n",
    "#         data_df.reset_index(inplace=True,drop=True)\n",
    "#         new_header = [sanitize_column_name(str(col)) for col in new_header]\n",
    "#         data_df.columns = new_header #set the header row as the df header\n",
    "#         save_name = full_file_name.split(\".\")[0].split(\"/\")[-1]\n",
    "#         save_file_path = os.path.join(os.path.join(processed_dir,country),save_name)\n",
    "#         data_df.to_csv(save_file_path+\".csv\",index=False)\n",
    "#         with open(save_file_path+\".json\", \"w\") as outfile: \n",
    "#             json.dump(info_dict, outfile)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:04:35.786185Z",
     "start_time": "2024-05-21T07:04:32.601566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# 设置 Pandas 显示选项\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "def sanitize_column_name(col_name):\n",
    "    # 移除特殊字符并替换空格为下划线\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "# 目录路径\n",
    "dir = \"DATA\"\n",
    "processed_dir = \"Processed Data\"\n",
    "all_infos_dict = []\n",
    "\n",
    "# 创建处理后的数据目录\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# 遍历每个国家的文件夹\n",
    "for country in os.listdir(dir):\n",
    "    print(country)\n",
    "    country_path = os.path.join(dir, country)\n",
    "    processed_country_path = os.path.join(processed_dir, country)\n",
    "    os.makedirs(processed_country_path, exist_ok=True)\n",
    "\n",
    "    # 遍历每个 Excel 文件\n",
    "    for excel_file in tqdm(os.listdir(country_path)):\n",
    "        full_file_name = os.path.join(country_path, excel_file)\n",
    "        xls = pd.ExcelFile(full_file_name)\n",
    "        sheet_names = xls.sheet_names\n",
    "\n",
    "        info_dict = {}\n",
    "        data_df = pd.DataFrame()\n",
    "\n",
    "        # 解析每个工作表\n",
    "        for sheet_name in sheet_names:\n",
    "            if \"Var\" in sheet_name or \"var\" in sheet_name:\n",
    "                info_df = xls.parse(sheet_name)\n",
    "                info_df.dropna(how=\"all\", inplace=True)\n",
    "                for cols in info_df.columns:\n",
    "                    if \"End\" not in cols and 'Unnamed' not in cols:\n",
    "                        info_dict['Summary'] = cols\n",
    "                info_dict['Vars'] = info_df.values[1:].tolist()\n",
    "                all_infos_dict.append(info_dict)\n",
    "            elif \"Industry\" in sheet_name or \"industry\" in sheet_name:\n",
    "                data_df = xls.parse(sheet_name)\n",
    "\n",
    "        try:\n",
    "            data_df.dropna(axis=1, thresh=5, inplace=True)\n",
    "            data_df.dropna(inplace=True)\n",
    "            new_header = data_df.iloc[0]  # 获取第一行作为表头\n",
    "            data_df = data_df[1:]  # 去掉表头行\n",
    "            data_df.reset_index(inplace=True, drop=True)\n",
    "            new_header = [sanitize_column_name(str(col)) for col in new_header]\n",
    "            data_df.columns = new_header  # 设置表头\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {full_file_name}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # 保存处理后的数据\n",
    "        save_name = os.path.splitext(excel_file)[0]\n",
    "        save_file_path = os.path.join(processed_country_path, save_name)\n",
    "        data_df.to_csv(save_file_path + \".csv\", index=False)\n",
    "        with open(save_file_path + \".json\", \"w\") as outfile:\n",
    "            json.dump(info_dict, outfile)\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A SAMPLE METADATA JSON"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:04:42.126208Z",
     "start_time": "2024-05-21T07:04:42.122146Z"
    }
   },
   "source": [
    "all_infos_dict[0]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATAFRAME AFTER PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:05:56.852294Z",
     "start_time": "2024-05-21T07:05:56.835545Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"Processed Data/US/capex.csv\")\n",
    "df.head()"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD TABLE NAMES AND METADATA\n",
    "\n",
    "* Here we use a DSPy signature given the first 10 rows of the dataframe, we generate the table name and table explanation. It will help us to dynamically select the correct table based on the query.(我们使用给定数据帧前 10 行的 DSPy 签名，生成表名称和表说明。 它将帮助我们根据查询动态选择正确的表。)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:06:44.378308Z",
     "start_time": "2024-05-21T07:06:44.374126Z"
    }
   },
   "source": [
    "df.head(10).to_csv()"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:35.809990Z",
     "start_time": "2024-05-21T08:21:35.803428Z"
    }
   },
   "source": [
    "load_dotenv(override=True)\n",
    "openai.api_key = api_configs.get('openai').get('api_key')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:38.890273Z",
     "start_time": "2024-05-21T08:21:38.883580Z"
    }
   },
   "source": [
    "# 这段代码展示了如何使用 DSPy 和 OpenAI API 来生成 SQL 表的元数据。通过定义 SQLTableMetadata 签名和 CoT 模块，可以将 pandas 数据框的前 10 行作为输入，生成合适的 SQL 表名和表的摘要。 \n",
    "import dspy\n",
    "turbo = dspy.OpenAI(model=api_configs.get('openai').get('model'), max_tokens=320)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "class SQLTableMetadata(dspy.Signature):\n",
    "    \"\"\"给定数据表，生成合适的表名和描述\"\"\"\n",
    "    pandas_dataframe_str = dspy.InputField(desc=\"First 10 rows of a pandas dataframe delimited by newline character\") # 前10行的 pandas 数据框，以换行符分隔\n",
    "    table_name = dspy.OutputField(desc=\"suitable table name\") # 合适的表名 \n",
    "    table_summary = dspy.OutputField(desc=\"a summary about the table\") # 表的摘要\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(SQLTableMetadata)\n",
    "    \n",
    "    def forward(self, pandas_dataframe_str):\n",
    "        return self.prog(pandas_dataframe_str=pandas_dataframe_str)\n",
    "\n",
    "cot = CoT()\n",
    "\n",
    "# cot(pandas_dataframe_str = df.head(10).to_csv())\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T07:38:27.322304Z",
     "start_time": "2024-05-21T07:18:57.788845Z"
    }
   },
   "source": [
    "processed_dir = \"Processed Data\"\n",
    "dfs_str = []\n",
    "for country in os.listdir(processed_dir):\n",
    "    country_folder = os.path.join(processed_dir,country)\n",
    "    # print(f\"{country}\")\n",
    "    for files in tqdm(os.listdir(country_folder),desc=f\"Building the summary and name for {country}\"):\n",
    "        if files.endswith(\".csv\"):\n",
    "            file_name = files.split(\".\")[0]\n",
    "            csv_file_path = os.path.join(country_folder,files)\n",
    "            df = pd.read_csv(csv_file_path,index_col=False)\n",
    "            json_file_path = os.path.join(country_folder,f\"{file_name}.json\")\n",
    "            with open(json_file_path,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            if 'table_name' in data and 'table_summary' in data:\n",
    "                # if data['table_name'] == \"\" or data['table_summary'] == \"\":\n",
    "                if data['table_summary'] == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "            dfs_str.append(df.head(10).to_csv())\n",
    "            table_preds = cot(pandas_dataframe_str = df.head(10).to_csv())\n",
    "            data['table_name'] = table_preds.table_name\n",
    "            data['table_summary'] = table_preds.table_summary\n",
    "            with open(json_file_path,'w') as f:\n",
    "                json.dump(data, f)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT TASKS\n",
    "1. Build database with each region for each table\n",
    "2. Embed the table summary and table SCHEMA. Also, embed the table rows\n",
    "3. Retrieval at table level and embed the rows to retrieve relevant rows from the retrieved schema of table\n",
    "4. Text-to-SQL pipeline\n",
    "\n",
    "1. 为每个表建立每个区域的数据库\n",
    "2. 嵌入表摘要和表SCHEMA。 另外，嵌入表格行\n",
    "3. 在表级别检索并嵌入行以从表的检索模式中检索相关行\n",
    "4. 文本到 SQL 管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD THE SQLITE DATABASE FROM THE CSV FILES (根据 csv 文件建立 sqlite 数据库)\n",
    "\n",
    "It was taken from the [tutorial](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql/)\n",
    "(它摘自 [教程]（https:docs.llamaindex.aienstableexamplespipelinequery_pipeline_sql）)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:48.050419Z",
     "start_time": "2024-05-21T08:21:48.045539Z"
    }
   },
   "source": [
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy (使用 SQLAlchemy 从 DataFrame 创建表格的函数)\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DATABASE CREATION (创建数据库)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:21:53.223293Z",
     "start_time": "2024-05-21T08:21:53.218864Z"
    }
   },
   "source": [
    "processed_dir  = \"Processed Data\"\n",
    "def sqlalchemy_engine(region:str):\n",
    "    \"\"\"Create a SQLAlchemy engine for the given region\"\"\"\n",
    "    assert region in os.listdir(processed_dir), f\"{region} is not a valid region from {os.listdir(processed_dir)}\"\n",
    "    # Create a SQLAlchemy database for each region\n",
    "    engine = create_engine(f\"sqlite:///{region}.db\")\n",
    "    metadata_obj = MetaData()\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    dfs = []\n",
    "    for dataframes_path in os.listdir(region_path):\n",
    "        if dataframes_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(region_path,dataframes_path),index_col=False)\n",
    "            dfs.append((dataframes_path,df))\n",
    "    pbar = tqdm(total=len(dfs),desc=f\"Creating tables for {region}\")\n",
    "    for _, df_table_name in enumerate(dfs):\n",
    "        table_name = df_table_name[0]\n",
    "        table_name = table_name.split(\".\")[0]\n",
    "        df = df_table_name[1]\n",
    "        # print(f\"Creating table: {table_name}\")\n",
    "        create_table_from_dataframe(df,table_name, engine, metadata_obj)\n",
    "        # print(f\"Done creating table for: {table_name}\")\n",
    "        pbar.update(1)\n",
    "    return engine"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:14.862419Z",
     "start_time": "2024-05-21T08:22:02.620087Z"
    }
   },
   "source": [
    "us_engine = sqlalchemy_engine(\"US\")\n",
    "india_engine = sqlalchemy_engine(\"India\")\n",
    "china_engine = sqlalchemy_engine(\"China\")\n",
    "europe_engine = sqlalchemy_engine(\"Europe\")\n",
    "global_engine = sqlalchemy_engine(\"Global\")\n",
    "aus_nz_canada_engine = sqlalchemy_engine(\"AUS_NZ_CANADA\")\n",
    "japan_engine = sqlalchemy_engine(\"Japan\")\n",
    "emerging_engine = sqlalchemy_engine(\"Emerging\")"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:19.794877Z",
     "start_time": "2024-05-21T08:22:19.789850Z"
    }
   },
   "source": [
    "\n",
    "def get_table_infos(sql_engine:sqlalchemy.engine.base.Engine,region:str):\n",
    "    \"\"\"Get all the tables info in the database based on the given region\"\"\"\n",
    "    inspector = inspect(sql_engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "    table_infos_dict = {tb: [] for tb in table_names}\n",
    "    for tb in table_names:\n",
    "        column_dict = inspector.get_columns(tb)\n",
    "        schema_str = \"\"\n",
    "        primary_keys = []\n",
    "        for col in column_dict:\n",
    "            schema_str += f\"{col['name']} ({col['type']}), \"\n",
    "            if col[\"primary_key\"] not in primary_keys:\n",
    "                primary_keys.append(col[\"name\"])\n",
    "        with open(os.path.join(processed_dir,region,f\"{tb}.json\")) as f:\n",
    "            table_info = json.loads(f.read())\n",
    "        table_infos_dict[tb] = [\n",
    "            {\n",
    "                \"table_info\": f\"Table {tb} has columns: {schema_str[:-2]}\",\n",
    "                \"table_summary\": f'{table_info.get(\"Summary\",None)}. {table_info[\"table_summary\"]}. ',\n",
    "            }\n",
    "        ]\n",
    "    return table_infos_dict"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:22.093025Z",
     "start_time": "2024-05-21T08:22:21.022547Z"
    }
   },
   "source": [
    "# 获取每一个类型中的每个表的详细信息\n",
    "us_tb_dict = get_table_infos(us_engine,\"US\")\n",
    "india_tb_dict = get_table_infos(india_engine,\"India\")\n",
    "china_tb_dict = get_table_infos(china_engine,\"China\")\n",
    "europe_tb_dict = get_table_infos(europe_engine,\"Europe\")\n",
    "global_tb_dict = get_table_infos(global_engine,\"Global\")\n",
    "aus_nz_canada_tb_dict = get_table_infos(aus_nz_canada_engine,\"AUS_NZ_CANADA\")\n",
    "japan_tb_dict = get_table_infos(japan_engine,\"Japan\")\n",
    "emerging_tb_dict = get_table_infos(emerging_engine,\"Emerging\")"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:27.054317Z",
     "start_time": "2024-05-21T08:22:27.049517Z"
    }
   },
   "cell_type": "code",
   "source": "us_tb_dict",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:28.030292Z",
     "start_time": "2024-05-21T08:22:28.026286Z"
    }
   },
   "source": [
    "us_tb_dict['DollarUS']"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS\n",
    "\n",
    "1. Embed the table summary and table SCHEMA to get the table that the user is looking for (1.嵌入表格摘要和表格 SCHEMA，以获取用户要查找的表格)\n",
    "2. Embed the table rows for each table, so as to get relevant rows from the retrieved table  (2.嵌入每个表的表行，以便从检索到的表中获取相关行)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## EMBED THE TABLE SUMMARY AND TABLE SCHEMA  (嵌入表格摘要和表格模式)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:31.410317Z",
     "start_time": "2024-05-21T08:22:31.383515Z"
    }
   },
   "source": [
    "# 使用了 chromadb 库和 OpenAI 的嵌入模型。这个函数的主要目的是将表的概要信息和表的模式（schema）嵌入到向量空间中，以便于后续的查询和匹配操作\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "\n",
    "load_dotenv(override=True)\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=api_configs.get('openai').get('api_key'),\n",
    "                model_name=\"text-embedding-3-small\")\n",
    "# EMBEDDING_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# emb_fn = embedding_functions.HuggingFaceEmbeddingFunction(model_name=EMBEDDING_MODEL,api_key=os.environ[\"HF_API_KEY\"])\n",
    "def embed_table_info(region:str,tb_dict):\n",
    "    \"\"\"Embed the table summary and table SCHEMA to get the table that the user is looking for\"\"\"\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "\n",
    "    table_collection = client.create_collection(name=\"table\",embedding_function=emb_fn)\n",
    "\n",
    "    table_docs = []\n",
    "    table_metadata = []\n",
    "\n",
    "\n",
    "    for table_name,table_data in tb_dict.items():\n",
    "        table_docs.append(table_data[0]['table_info'] + \". \" + table_data[0]['table_summary'])\n",
    "        table_metadata.append({\"table_name\":table_name,'table_metadata':table_data[0]['table_info']})\n",
    "    table_ids = [f\"id{i}\" for i in range(len(table_docs))]\n",
    "    assert len(table_docs) == len(table_metadata)\n",
    "    print(len(table_docs),len(table_metadata))\n",
    "    # Create a batch of data to be sent to OpenAI Embedding API\n",
    "    batches = create_batches(api=client,ids=table_ids, documents=table_docs, metadatas=table_metadata)\n",
    "    for batch in tqdm(batches,desc=\"Embedding table info\"):\n",
    "        table_collection.add(ids=batch[0],\n",
    "                    documents=batch[3],\n",
    "                    metadatas=batch[2])\n",
    "\n",
    "# embed_table_info(\"US\",us_tb_dict)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## For some strange reason, the `create_batches` was not batching the below documents, hence I had to do it manually (由于某些奇怪的原因，\"create_batches \"没有对以下文件进行批处理，因此我不得不手动操作)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:32.703464Z",
     "start_time": "2024-05-21T08:22:32.698278Z"
    }
   },
   "source": [
    "# 使用了 chromadb 库和 OpenAI 的嵌入模型。这个函数的主要目的是将表格中的每一行数据嵌入到向量空间中，以便于后续的查询和匹配操作。以下是该代码的详细解析：\n",
    "def embed_rows(region:str,batch_size:int=24):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "    # client.delete_collection(name=\"rows\")\n",
    "    rows_collection = client.create_collection(name=\"rows\",embedding_function=emb_fn)\n",
    "\n",
    "    rows_docs = []\n",
    "    rows_metadata = []\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    for df_path in os.listdir(region_path):\n",
    "        df_full_path = os.path.join(region_path,df_path)\n",
    "        df = pd.read_csv(df_full_path,index_col=False)\n",
    "        for idx,row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            full_rows = []\n",
    "            for rv in row.values:\n",
    "                if isinstance(rv,str):\n",
    "                    row_str+= rv + \", \"\n",
    "                full_rows.append(str(rv))\n",
    "                row_str = row_str.replace('\"',\"\")\n",
    "                # row_str = row_str.replace(\"'\",'\"')\n",
    "            full_rows_str = \", \".join(full_rows)[:-2]\n",
    "            full_rows_str = full_rows_str.replace('\"',\"\")\n",
    "            rows_docs.append(row_str[:-2])\n",
    "            rows_metadata.append({\"table_name\":df_path.split(\".\")[0],\"region\":region,\"index\":idx,\"full_rows\":full_rows_str})\n",
    "    row_ids = [f\"id{i}\" for i in range(len(rows_docs))]\n",
    "    # print(len(rows_docs),len(rows_metadata))\n",
    "    assert len(rows_docs) == len(rows_metadata) == len(row_ids)\n",
    "    # return rows_docs,rows_metadata,row_ids\n",
    "    for start in tqdm(range(0,len(rows_docs),batch_size),desc=\"Embedding rows\"):\n",
    "        end = min(start+batch_size,len(rows_docs))\n",
    "        batch_ids = row_ids[start:end]\n",
    "        batch_rows = rows_docs[start:end]\n",
    "        batch_metadatas = rows_metadata[start:end]\n",
    "        rows_collection.add(ids=batch_ids,\n",
    "                    documents=batch_rows,\n",
    "                    metadatas=batch_metadatas)\n",
    "    # return batches"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:22:48.741091Z",
     "start_time": "2024-05-21T08:22:48.687394Z"
    }
   },
   "source": [
    "region = \"US\"\n",
    "embed_table_info(region,us_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:35:27.888149Z",
     "start_time": "2024-05-21T08:35:27.829990Z"
    }
   },
   "source": [
    "region = \"India\"\n",
    "embed_table_info(region,india_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:23:40.570869Z",
     "start_time": "2024-05-21T08:23:25.793996Z"
    }
   },
   "source": [
    "region = \"China\"\n",
    "embed_table_info(region,china_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:35:20.418071Z",
     "start_time": "2024-05-21T08:35:06.236767Z"
    }
   },
   "source": [
    "region = \"Europe\"\n",
    "embed_table_info(region,europe_tb_dict)\n",
    "embed_rows(region,1000)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:35:53.517072Z",
     "start_time": "2024-05-21T08:35:37.796894Z"
    }
   },
   "source": [
    "region = \"Global\"\n",
    "embed_table_info(region,global_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:36:06.653263Z",
     "start_time": "2024-05-21T08:35:53.518617Z"
    }
   },
   "source": [
    "region = \"Emerging\"\n",
    "embed_table_info(region,emerging_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:36:20.121621Z",
     "start_time": "2024-05-21T08:36:06.654515Z"
    }
   },
   "source": [
    "region = \"Japan\"\n",
    "embed_table_info(region,japan_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:36:34.312529Z",
     "start_time": "2024-05-21T08:36:20.123152Z"
    }
   },
   "source": [
    "region = \"AUS_NZ_CANADA\"\n",
    "embed_table_info(region,aus_nz_canada_tb_dict)\n",
    "embed_rows(region,2000)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT-TO-SQL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:41:45.659668Z",
     "start_time": "2024-05-21T08:41:45.654944Z"
    }
   },
   "source": [
    "db_dict = {\n",
    "    \"US\":us_engine,\n",
    "    \"India\":india_engine,\n",
    "    \"China\":china_engine,\n",
    "    \"Europe\":europe_engine,\n",
    "    \"Global\":global_engine,\n",
    "    \"AUS_NZ_CANADA\":aus_nz_canada_engine,\n",
    "    \"Japan\":japan_engine,\n",
    "    \"Emerging\":emerging_engine,\n",
    "}\n",
    "\n",
    "def get_collections_db(region:str):\n",
    "    # Get the database for the given region, table collection and row collection\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "    table_collection = client.get_collection(name=\"table\",embedding_function=emb_fn)\n",
    "    row_collection = client.get_collection(name=\"rows\",embedding_function=emb_fn)\n",
    "    return [db_dict[region],table_collection,row_collection]"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:43:05.251563Z",
     "start_time": "2024-05-21T08:43:05.195136Z"
    }
   },
   "source": [
    "db_collection_dict = {\n",
    "    \"US\":get_collections_db(\"US\"),\n",
    "    \"India\":get_collections_db(\"India\"),\n",
    "    \"China\":get_collections_db(\"China\"),\n",
    "    \"Europe\":get_collections_db(\"Europe\"),\n",
    "    \"Global\":get_collections_db(\"Global\"),\n",
    "    \"AUS_NZ_CANADA\":get_collections_db(\"AUS_NZ_CANADA\"),\n",
    "    \"Japan\":get_collections_db(\"Japan\"),\n",
    "    \"Emerging\":get_collections_db(\"Emerging\"),\n",
    "}"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:44:59.269425Z",
     "start_time": "2024-05-21T08:44:59.246555Z"
    }
   },
   "source": [
    "load_dotenv(override=True)\n",
    "text_to_sql = dspy.OpenAI(model=api_configs.get('openai').get('model'), max_tokens=1024)\n",
    "sql_to_answer = dspy.OpenAI(model=api_configs.get('openai').get('model'),max_tokens=1024)\n",
    "\n",
    "# DSPy signature for converting text to SQL query (# 将文本转换为 SQL 查询的 DSPy 签名)\n",
    "# \n",
    "class TextToSQLAnswer(dspy.Signature):\n",
    "    \"\"\"Convert natural language text to SQL using suitable schema(s) from multiple schema choices\"\"\"\n",
    "\n",
    "    question:str = dspy.InputField(desc=\"natural language input which will be converted to SQL\")\n",
    "    relevant_table_schemas_rows:str = dspy.InputField(desc=\"Multiple possible tables which has table name and corresponding columns, along with relevant rows from the table (values in the same order as columns above)\")\n",
    "    sql:str = dspy.OutputField(desc=\"Generate syntactically correct sqlite query with correct column names using suitable tables(s) and its rows.\\n Don't forget to add distinct.\\n Please rename the returned columns into suitable names.\\n DON'T OUTPUT anything else other than the sqlite query\")\n",
    "\n",
    "# DSPy signature for converting SQL query and question to natural language text\n",
    "class SQLReturnToAnswer(dspy.Signature):\n",
    "    \"\"\"Answer the question using the rows from the SQL query\"\"\"\n",
    "    question:str = dspy.InputField()\n",
    "    sql:str = dspy.InputField(desc=\"sqlite query that generated the rows\")\n",
    "    relevant_rows:str = dspy.InputField(desc=\"relevant rows to answer the question\")\n",
    "    answer:str = dspy.OutputField(desc=\"answer to the question using relevant rows and the sql query\")\n",
    "\n",
    "# If there is an SQLError, then rectify the error by trying again\n",
    "class SQLRectifier(dspy.Signature):\n",
    "    \"\"\"Correct the SQL query to resolve the error using the proper table names, columns and rows\"\"\"  \n",
    "    input_sql:str = dspy.InputField(desc=\"sqlite query that needs to be fixed\")\n",
    "    error_str: str = dspy.InputField(desc=\"error that needs to be resolved\")\n",
    "    relevant_table_schemas_rows:str = dspy.InputField(desc=\"Multiple possible tables which has table name and corresponding columns, along with relevant rows from the table (values in the same order as columns above)\")\n",
    "    sql:str = dspy.OutputField(desc=\"corrected sqlite query to resolve the error and remove and any invalid syntax in the query.\\n Don't output anything else other than the sqlite query\")\n",
    "\n",
    "dspy.settings.configure(lm=text_to_sql)\n",
    "\n",
    "# Filter out the SQL Query\n",
    "def process_sql_str(sql_str:str):\n",
    "    sql_str = sql_str.replace(\"```\",\"\")\n",
    "    sql_str = sql_str.replace(\"sql\",\"\")\n",
    "    sql_str = sql_str.strip()\n",
    "    return sql_str"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Athe-kunal/Text-to-SQL/main/Schema.png\" alt=\"Sublime's custom image\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:46:33.644402Z",
     "start_time": "2024-05-21T08:46:33.640386Z"
    }
   },
   "source": [
    "# 用于在指定的表集合和行集合中查询与给定问题相关的结果\n",
    "def get_table_results(table_collection_,question:str):\n",
    "    # question_emb = emb_fn.embed_with_retries(question)[0]\n",
    "    # Get the table results for the given question\n",
    "    table_results = table_collection_.query(\n",
    "        query_texts = question,\n",
    "        n_results = 5\n",
    "    )\n",
    "    # print(table_results['documents'][0])\n",
    "    return table_results\n",
    "\n",
    "def get_row_results(row_collection_,question,table_name:str):\n",
    "    # Get the row results for the given question\n",
    "    row_results = row_collection_.query(\n",
    "        query_texts = question,\n",
    "        where = {\"table_name\":table_name},\n",
    "        n_results = 5\n",
    "    )\n",
    "    print(row_results['documents'][0])\n",
    "    return row_results"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:51:32.340294Z",
     "start_time": "2024-05-21T08:51:30.907526Z"
    }
   },
   "source": [
    "from typing import Any\n",
    "import re\n",
    "\n",
    "class TextToSQLQueryModule(dspy.Module):\n",
    "    \"\"\"Text to SQL to final module\"\"\"\n",
    "    def __init__(self,region:str,use_cot:bool=True,max_retries:int=3):\n",
    "        \"\"\"Text to Answer init module\n",
    "\n",
    "        Args:\n",
    "            region (str): Region for which the module will be used.\n",
    "            use_cot (bool, optional): Whether to use chain of thought for sql query generation. Defaults to True.\n",
    "            max_retries (int, optional): Number of max retries for SQLError. Defaults to 3.\n",
    "        在初始化时设置区域、数据库连接、表集合、行集合等参数。\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.region = region\n",
    "        db,table_collection,row_collection = db_collection_dict[region]\n",
    "        # print(db,table_collection,row_collection)\n",
    "        self.table_collection = table_collection\n",
    "        self.use_cot = use_cot\n",
    "        self.db = db\n",
    "        self.row_collection = row_collection\n",
    "        if self.use_cot == True:\n",
    "            self.sqlAnswer = dspy.ChainOfThought(TextToSQLAnswer)\n",
    "        else:\n",
    "            self.sqlAnswer = dspy.Predict(TextToSQLAnswer)\n",
    "        self.final_output = dspy.Predict(SQLReturnToAnswer)\n",
    "        self.max_tries = max_retries\n",
    "        # Initialize the sql rectifier with CoT reasoning\n",
    "        self.sql_rectifier = dspy.ChainOfThought(SQLRectifier,rationale_type=dspy.OutputField(\n",
    "            prefix=\"reasoning: let's think step by step in order to\",\n",
    "            desc=\"${produce the answer}. We ...\"\n",
    "        ))\n",
    "    \n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        return self.forward(*args, **kwargs)\n",
    "        \n",
    "    def forward(self,question):\n",
    "        \"\"\"\n",
    "            \n",
    "    `forward` 函数通过以下步骤实现了从自然语言问题到 SQL 查询再到最终答案的转换：\n",
    "    1. 将问题嵌入到向量空间中。\n",
    "    2. 检索与问题相关的表格和行数据。\n",
    "    3. 生成 SQL 查询。\n",
    "    4. 执行 SQL 查询并处理可能的异常。\n",
    "    5. 处理查询结果并生成最终答案。 \n",
    "        \"\"\"\n",
    "        # Embed the question with embedding function\n",
    "        question_emb = emb_fn([question])[0]\n",
    "        # Retrieve the relevant tables from table schema and table summary\n",
    "        docs = self.table_collection.query(\n",
    "            query_embeddings = question_emb,\n",
    "            n_results = 5\n",
    "        )\n",
    "        # docs = get_table_results(db_collection_dict[self.region][1],question)\n",
    "        relevant_rows_schemas = \"\"\n",
    "    \n",
    "        existing_table_names = []\n",
    "\n",
    "        for table_idx,metadata_name in enumerate(docs['metadatas'][0]):\n",
    "            table_metadata = metadata_name['table_metadata']\n",
    "            table_name = metadata_name['table_name']\n",
    "            # If the table name is already in the list of existing table names, skip it\n",
    "            # if table_name in existing_table_names: \n",
    "            #     continue\n",
    "            existing_table_names.append(table_name)\n",
    "            # Retrieve the relevant rows from the current table\n",
    "            rows = self.row_collection.query(\n",
    "                query_embeddings = question_emb,\n",
    "                n_results = 5,\n",
    "                # where clause to filter the rows\n",
    "                where = {\"table_name\":table_name}\n",
    "            )\n",
    "            # Retrieve the relevant table with the schema and summary\n",
    "            relevant_rows_schemas += f'Table name: {table_name} \\n'\n",
    "            relevant_rows_schemas += \"/* \\n\"\n",
    "            for match in re.finditer(\"columns: \",table_metadata):\n",
    "                cols_end = match.end()\n",
    "            relevant_rows_schemas += \"col : \" + \" | \".join(table_metadata[cols_end:].split(\", \")) + \"\\n\"\n",
    "            for row_idx,row in enumerate(rows['metadatas'][0]):\n",
    "                # Get the relevant rows from the current table\n",
    "                # relevant_rows_schemas += f'\\tRow {row_idx+1} from table {table_name}: {row[\"full_rows\"]}\\n'\n",
    "                relevant_rows_schemas += f'row {row_idx+1} : {\" | \".join(row[\"full_rows\"].split(\", \"))}\\n'\n",
    "            relevant_rows_schemas += \"*/\" + '\\n\\n'\n",
    "        print(colored(relevant_rows_schemas,\"yellow\"))\n",
    "        # return \n",
    "        sql_query = self.sqlAnswer(question=question,relevant_table_schemas_rows=relevant_rows_schemas)\n",
    "\n",
    "        num_tries = 0\n",
    "        print(sql_query)\n",
    "        while num_tries <= self.max_tries:\n",
    "            with self.db.connect() as conn:\n",
    "                try:\n",
    "                    # Try executing the sql query for the database\n",
    "                    result = conn.execute(text(process_sql_str(sql_query.sql)))\n",
    "                    num_tries = self.max_tries + 1\n",
    "                except Exception as error:\n",
    "                    # If there is an sql error, then try again with the sql rectifier\n",
    "                    print(colored(str(error),'red'))\n",
    "                    sql_query = self.sql_rectifier(input_sql=sql_query.sql,error_str=str(error),relevant_table_schemas_rows=relevant_rows_schemas)\n",
    "                    print(colored(sql_query.rationale,'green'))\n",
    "                    print()\n",
    "                    print(colored(sql_query.sql,'green'))\n",
    "                    # If all the num_retries are exhausted, then exit the program\n",
    "                    num_tries += 1\n",
    "                    if num_tries == self.max_tries+1:\n",
    "                        return sql_query,error\n",
    "        # With the retrieved rows from the database, then try to answer the question with dspy context\n",
    "        with dspy.context(lm=sql_to_answer):\n",
    "            row_str = \"\"\n",
    "            key = tuple(result.keys())\n",
    "            for row in result.fetchall():\n",
    "                for r,k in zip(row,key):\n",
    "                    row_str += f\" {k} = {r},\"\n",
    "                row_str = row_str[:-1]\n",
    "                row_str += \"\\n\"\n",
    "            print(f\"Extracted rows: {row_str}\")\n",
    "            final_answer = self.final_output(question=question,sql=sql_query.sql,relevant_rows=row_str)\n",
    "            return final_answer\n",
    "tsql_ = TextToSQLQueryModule(\"US\")\n",
    "question = \"What is the ebitda of software and packaging industry?\" \n",
    "# \"EBITDA\" 是 \"Earnings Before Interest, Taxes, Depreciation, and Amortization\" 的缩写，中文翻译为“息税折旧摊销前利润”。这是一种常用的财务指标，用于衡量公司的经营业绩，不包括利息、税项、折旧和摊销的影响。\n",
    "# 软件和包装行业的息税折旧摊销前利润是多少？\n",
    "sq = tsql_(question = question)"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:51:40.189577Z",
     "start_time": "2024-05-21T08:51:40.186406Z"
    }
   },
   "source": [
    "print(sq)"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:19:45.755204Z",
     "start_time": "2024-05-21T09:19:34.779441Z"
    }
   },
   "source": [
    "# tsql = TextToSQLQueryModule(\"US\")\n",
    "# sq = tsql(question=\"What is the effective tax rate of the healthcare industry?\")\n",
    "sq = tsql_(question=\"What is the EBITDA value and number of firms for all the Software industries, semiconductor industry and aerospace?\")\n",
    "# 所有软件行业、半导体行业和航空航天行业的息税折旧摊销前利润（EBITDA）值和公司数量是多少？\n",
    "# sq = tsql(\"What is the debt to EBITDA ratio for software industry?\")"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:22:02.996422Z",
     "start_time": "2024-05-21T09:22:02.993034Z"
    }
   },
   "source": [
    "print(sq.answer)\n",
    "#贝塔值（Beta Value）是一个衡量股票或投资组合相对于整个市场波动性的指标。具体来说，它表示某个资产的价格变动相对于市场整体价格变动的敏感程度。贝塔值在金融学中是资本资产定价模型（CAPM，Capital Asset Pricing Model）中的一个重要参数，用于评估系统性风险"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "tsql = TextToSQLQueryModule(\"India\")\n",
    "sq = tsql(question=\"What is the beta value and number of firms for all the Software industries and semiconductor industry?\")\n",
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:23:05.397846Z",
     "start_time": "2024-05-21T09:23:04.454079Z"
    }
   },
   "source": [
    "tsql = TextToSQLQueryModule(\"China\")\n",
    "sq = tsql(question=\"What is the beta value and number of firms for all the Software industries and semiconductor industry?\")\n",
    "# 所有软件行业和半导体行业的贝塔值和公司数量是多少？\n",
    "print(sq.answer)"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:23:07.364869Z",
     "start_time": "2024-05-21T09:23:07.361639Z"
    }
   },
   "source": [
    "print(sq.answer)"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "tsql = TextToSQLQueryModule(\"US\")\n",
    "sq = tsql(question=\"What is the average tax rate of all healthcare industries?\")\n",
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "source": [
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "source": [
    "sq = tsql(question=\"Give me the average tax rate of all healthcare industries where revenues per employee is more than 1 million?\")\n",
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "source": [
    "tsql = TextToSQLQueryModule(\"Europe\")\n",
    "sq = tsql(question=\"Give me the average tax rate of all banking industries where number of firms is more than 500?\")\n",
    "print(sq.answer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
