/home/cc/miniconda/lib/python3.12/site-packages/langchain/tools/__init__.py:63: LangChainDeprecationWarning: Importing tools from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:

`from langchain_community.tools import DuckDuckGoSearchRun`.

To install langchain-community run `pip install -U langchain-community`.
  warnings.warn(
2024-06-19 14:06:58,099 - 140568024827456 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.8s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,118 - 140568016434752 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.5s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,173 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,194 - 140567998600768 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,644 - 140568016434752 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.9s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,865 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,896 - 140568024827456 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.1s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:58,926 - 140567998600768 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.6s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:06:59,524 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 3.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:00,027 - 140568024827456 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.6s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:00,544 - 140568016434752 - _common.py-_common:105 - INFO: Backing off send_request(...) for 2.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:00,556 - 140567998600768 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:01,592 - 140568024827456 - _common.py-_common:120 - ERROR: Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:02,257 - 140567998600768 - _common.py-_common:120 - ERROR: Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:03,226 - 140568016434752 - _common.py-_common:120 - ERROR: Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:07:03,256 - 140568006993472 - _common.py-_common:120 - ERROR: Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
{'agents': [{'name': 'video_processor', 'role': 'Video Processor', 'goal': 'Extract audio from video and convert it to text using Whisper', 'backstory': 'You are an expert in handling multimedia files. Your primary task is to process video files and accurately transcribe the audio content into text.\n', 'verbose': True, 'allow_delegation': False, 'tools': ['whisper_translate_audio']}, {'name': 'text_summarizer', 'role': 'Text Summarizer', 'goal': 'Summarize transcribed text using RAG (Retrieval-Augmented Generation)', 'backstory': 'You specialize in analyzing and summarizing large volumes of text. Your goal is to distill the most important information from the transcribed text.\n\nIf the current article exceeds 7,000 words, divide it into smaller chunks and then proceed with the query.\n', 'verbose': True, 'allow_delegation': False, 'tools': ['TXTSearchTool']}], 'tasks': [{'description': "Extract audio from the provided video file and transcribe it to text using Whisper. Ensure that the transcription is accurate and complete. Save the transcription to a file.\n\naudio_file: '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/input/Andrew Ng of AI Fund.mp3'\nmodule_file: 'medium'\nlanguage: English\nsave_file: '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt'\n", 'expected_output': 'Transcribed text from video saved to file', 'agent': 'video_processor', 'max_inter': 1}, {'description': "Using RAG, summarize the text. Ensure the summary captures the key points and relevant details from the content. Finally, produce a well-organized and comprehensive article.\nuse TXTSearchTool tool rag  \nIf there are a lot of repetitions, only the meaningful text is summarized.(For example, the key words and their meanings)  All steps are in English\nfile_path:  '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt'\n", 'expected_output': 'Summarized text to report ', 'agent': 'text_summarizer', 'max_inter': 1}], 'model': {'model_api_key': '', 'model_name': 'gpt-4', 'model_max_tokens': 2048, 'module_api_url': None}, 'other': {'proxy_url': 'http://192.168.0.75:10809'}, 'env': {'SERPER_API_KEY': '2e4d9ef16c251219c58cb2b04509d626e43b09c1'}, 'crewai_config': {'memory': True}}
[1m[95m [DEBUG]: == Working Agent: Video Processor[00m
[1m[95m [INFO]: == Starting Task: Extract audio from the provided video file and transcribe it to text using Whisper. Ensure that the transcription is accurate and complete. Save the transcription to a file.

audio_file: '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/input/Andrew Ng of AI Fund.mp3'
module_file: 'medium'
language: English
save_file: '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt'
[00m


[1m> Entering new CrewAgentExecutor chain...[0m
[32;1m[1;3mI need to transcribe the audio content from the given audio file into text using the Whisper model. To do this, I will use the whisper_translate_audio function. 

Action: whisper_translate_audio
Action Input: {"audio_file": "/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/input/Andrew Ng of AI Fund.mp3", "module_file": "medium", "language": "English", "save_file": "/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt"}[0m[95m 

 All of you know Andrew Ng as a famous computer science professor at Stanford, was really early on in the development of neural networks with GPUs. Of course, a creator of Coursera and popular courses like deeplearning.ai, also the founder and creator and early lead of Google Brain. But one thing I've always wanted to ask you before I hand it over Andrew while you're on stage is a question I think would be relevant to the whole audience. Ten years ago on problem set number two of CS229, you gave me a B. And I was wondering, I looked it over, I was wondering what you saw that I did incorrectly. So anyway, Andrew. Thank you, Hansin. I'm looking forward to sharing with all of you what I'm seeing with AI agents, which I think is an exciting trend that I think everyone building in AI should pay attention to. And then also excited about all the other what's next presentations. So AI agents, today the way most of us use large language models is like this, with a non-agentic workflow where you type a prompt and generates an answer. And that's a bit like if you ask a person to write an essay on a topic and I say, please sit down at the keyboard and just type the essay from start to finish without ever using backspace. And despite how hard this is, LLMs do it remarkably well. In contrast with an agentic workflow, this is what it may look like. Have an AI, have an LLM, say write an essay outline. Do you need to do any web research? If so, let's do that. Then write the first draft and then read your own draft and think about what parts need revision and then revise your draft and you go on and on. And so this workflow is much more iterative where you may have the LLM do some thinking and then revise this article and then do some more thinking and iterate this through a number of times. And what not many people appreciate is this delivers remarkably better results. I've actually really surprised myself working with these agent workflows, how well they work. I want to do one case study. My team analyzed some data using a coding benchmark called the human eval benchmark released by OpenAI a few years ago. But this says coding problems like given the non-NT list of integers return the sum of all the odd elements or uneven positions. And it turns out the answer is code snippet like that. So today, a lot of us will use zero-shot prompting, meaning we tell the AI write the code and have it run on the first pass. Like who codes like that? No human codes like that. We just type out the code and run it. Maybe you do. But you can't do that. So it turns out that if you use GPT 3.5, zero-shot prompting, it gets it 48% right. GPT 4, way better, 67% right. But if you take an agent workflow and wrap it around GPT 3.5, say, it actually does better than even GPT 4. And if you were to wrap this type of workflow around GPT 4, it also does very well. And you notice that GPT 3.5 with an agent workflow actually outperforms GPT 4. And I think this means that this has significant consequences for how we all approach building applications. So agents is a term that has been tossed around a lot. There's a lot of consultant reports, how about agents, the future of AI, blah, blah, blah. I want to be a bit concrete and share with you the broad design patterns I'm seeing in agents. It's a very messy, chaotic space. Tons of research, tons of open source. There's a lot going on. But I try to categorize a bit more concretely what's going on in agents. Reflection is a tool that I think many of us should just use. It just works. Tool use, I think it's more widely appreciated. But it actually works pretty well. I think of these as pretty robust technologies. When I use them, I can almost always get them to work well. Planning and multi-agent collaboration, I think is more emerging. When I use them, sometimes my mind is blown for how well they work. But at least at this moment in time, I don't feel like I can always get them to work reliably. So let me walk through these four design patterns in a few slides. And if some of you go back and yourself will ask your engineers to use these, I think you get a productivity boost quite quickly. So reflection, here's an example. Let's say I ask a system, please write code for me for a given task. Then we have a coded agent, just an LLM that you prompt to write code to say, yo, def, do task, write a function like that. An example of self-reflection would be if you then prompt the LLM with something like this. Here's code intended for a task and just give it back the exact same code that you just generated. And then say, check the code carefully for correctness, sound, efficiency, good construction for them. Just write a prompt like that. It turns out the same LLM that you prompted to write the code may be able to spot problems like this bug in line five, we fix it, blah, blah, blah. And if you now take his own feedback and give it to it and reprompt it, it may come up with a version two of the code that could well work better than the first version. Not guaranteed, but it works often enough for this to be worth trying for a lot of applications. To foreshadow tool use, if you let it run unit tests, if it fails a unit test, then why did you fail the unit test? Have that conversation and maybe they'll figure out, fail the unit test, so you should try changing something and come up with v3. By the way, for those of you that want to learn more about these technologies, I'm very excited about them. For each of the four sections, I have a little recommended reading section in the bottom that hopefully gives more references. And again, just to foreshadow multi-agent systems, I've described as a single coder agent that you prompt to have this conversation with itself. One natural evolution of this idea is instead of a single coder agent, you can have two agents where one is a coder agent and the second is a critic agent. And these could be the same base LLM model, but you prompt in different ways. We say one, you're an expert coder, write code. The other one is say you're an expert code reviewer, review this code. And this type of workflow is actually pretty easy to implement. I think it's such a very general purpose technology for a lot of workflows. This would give you a significant boost in the performance of LLMs. The second design pattern is tool use. Many of you will already have seen LLM-based systems using tools. On the left is a screenshot from copilot. On the right is something that I extracted from GPT-4. But LLMs today, if you ask it, what's the best coffee maker in your web search for some problems, LLMs will generate code and run codes. And it turns out that there are a lot of different tools that many different people are using for analysis, for gathering information, for taking action, for personal productivity. It turns out a lot of the early work in tool use turned out to be in the computer vision community because before large language models, LLMs, they couldn't do anything with images. So the only option was to generate a function call that could manipulate an image, like generate an image or do object detection or whatever. So if you actually look at literature, it's been interesting how much of the work in tool seems like it originated from vision because LLMs were blind to images before GPT-4V and Lava and so on. So that's tool use and it expands what an LLM can do. And then planning. For those of you that have not yet played a lot with planning algorithms, I feel like a lot of people talk about the chat GPT moment where you're, wow, never seen anything like this. I think you've not used planning algorithms. Many people will have a kind of AI agent, wow, I couldn't imagine an AI agent doing this. So I've run live demos where something failed and the AI agent rerouted around the failures. I've actually had quite a few of those moments where, wow, I can't believe my AI system just did that autonomously. But one example that I adapted from a hugging GPT paper, you say, please generate an image where a girl is reading a book and the pose is the same as the boy in the image, example.jpg And please describe the new image for the boy. So give an example like this. Today with AI agents, you can kind of decide, first thing I need to do is determine the pose of the boy. Then find the right model, maybe on hugging face, to extract the pose. Then next, you need to find the post image model to synthesize a picture of a girl as following the instructions. Then use image to text. And then finally use text to speech. And today we actually have agents that I don't want to say they work reliably. They're kind of finicky. They don't always work. But when it works, it's actually pretty amazing. But with agent groups, sometimes you can recover from earlier failures as well. So I find myself already using research agents for some of my work. Well, one piece of research, but I don't feel like Googling myself and spend a long time. I should send to the research agent, come back in a few minutes and see what it's come up with. And it sometimes works, sometimes it doesn't. But that's already part of my personal workflow. The final design pattern, multi-agent collaboration. This is one of those funny things. But it works much better than you might think. But on the left is a screenshot from a paper called Chat Dev, which is actually open source. Many of you saw the flashy social media announcements of demo of a devin. Chat Dev is open source, it runs on my laptop. And what Chat Dev does is example of a multi-agent system where you prompt one LLM to sometimes act like the CEO of a software engine company, sometimes act like a designer.
[00m
[32;1m[1;3mThought: I have successfully transcribed the audio content from the given audio file into text using the Whisper model and saved it to the specified location. This is the transcribed text, which is the final answer.
Final Answer: All of you know Andrew Ng as a famous computer science professor at Stanford, was really early on in the development of neural networks with GPUs. Of course, a creator of Coursera and popular courses like deeplearning.ai, also the founder and creator and early lead of Google Brain. But one thing I've always wanted to ask you before I hand it over Andrew while you're on stage is a question I think would be relevant to the whole audience. Ten years ago on problem set number two of CS229, you gave me a B. And I was wondering, I looked it over, I was wondering what you saw that I did incorrectly. So anyway, Andrew. Thank you, Hansin. I'm looking forward to sharing with all of you what I'm seeing with AI agents, which I think is an exciting trend that I think everyone building in AI should pay attention to. And then also excited about all the other what's next presentations. So AI agents, today the way most of us use large language models is like this, with a non-agentic workflow where you type a prompt and generates an answer. And that's a bit like if you ask a person to write an essay on a topic and I say, please sit down at the keyboard and just type the essay from start to finish without ever using backspace. And despite how hard this is, LLMs do it remarkably well. In contrast with an agentic workflow, this is what it may look like. Have an AI, have an LLM, say write an essay outline. Do you need to do any web research? If so, let's do that. Then write the first draft and then read your own draft and think about what parts need revision and then revise your draft and you go on and on. And so this workflow is much more iterative where you may have the LLM do some thinking and then revise this article and then do some more thinking and iterate this through a number of times. And what not many people appreciate is this delivers remarkably better results. I've actually really surprised myself working with these agent workflows, how well they work. I want to do one case study. My team analyzed some data using a coding benchmark called the human eval benchmark released by OpenAI a few years ago. But this says coding problems like given the non-NT list of integers return the sum of all the odd elements or uneven positions. And it turns out the answer is code snippet like that. So today, a lot of us will use zero-shot prompting, meaning we tell the AI write the code and have it run on the first pass. Like who codes like that? No human codes like that. We just type out the code and run it. Maybe you do. But you can't do that. So it turns out that if you use GPT 3.5, zero-shot prompting, it gets it 48% right. GPT 4, way better, 67% right. But if you take an agent workflow and wrap it around GPT 3.5, say, it actually does better than even GPT 4. And if you were to wrap this type of workflow around GPT 4, it also does very well. And you notice that GPT 3.5 with an agent workflow actually outperforms GPT 4. And I think this means that this has significant consequences for how we all approach building applications. So agents is a term that has been tossed around a lot. There's a lot of consultant reports, how about agents, the future of AI, blah, blah, blah. I want to be a bit concrete and share with you the broad design patterns I'm seeing in agents. It's a very messy, chaotic space. Tons of research, tons of open source. There's a lot going on. But I try to categorize a bit more concretely what's going on in agents. Reflection is a tool that I think many of us should just use. It just works. Tool use, I think it's more widely appreciated. But it actually works pretty well. I think of these as pretty robust technologies. When I use them, I can almost always get them to work well. Planning and multi-agent collaboration, I think is more emerging. When I use them, sometimes my mind is blown for how well they work. But at least at this moment in time, I don't feel like I can always get them to work reliably. So let me walk through these four design patterns in a few slides. And if some of you go back and yourself will ask your engineers to use these, I think you get a productivity boost quite quickly. So reflection, here's an example. Let's say I ask a system, please write code for me for a given task. Then we have a coded agent, just an LLM that you prompt to write code to say, yo, def, do task, write a function like that. An example of self-reflection would be if you then prompt the LLM with something like this. Here's code intended for a task and just give it back the exact same code that you just generated. And then say, check the code carefully for correctness, sound, efficiency, good construction for them. Just write a prompt like that. It turns out the same LLM that you prompted to write the code may be able to spot problems like this bug in line five, we fix it, blah, blah, blah. And if you now take his own feedback and give it to it and reprompt it, it may come up with a version two of the code that could well work better than the first version. Not guaranteed, but it works often enough for this to be worth trying for a lot of applications. To foreshadow tool use, if you let it run unit tests, if it fails a unit test, then why did you fail the unit test? Have that conversation and maybe they'll figure out, fail the unit test, so you should try changing something and come up with v3. By the way, for those of you that want to learn more about these technologies, I'm very excited about them. For each of the four sections, I have a little recommended reading section in the bottom that hopefully gives more references. And again, just to foreshadow multi-agent systems, I've described as a single coder agent that you prompt to have this conversation with itself. One natural evolution of this idea is instead of a single coder agent, you can have two agents where one is a coder agent and the second is a critic agent. And these could be the same base LLM model, but you prompt in different ways. We say one, you're an expert coder, write code. The other one is say you're an expert code reviewer, review this code. And this type of workflow is actually pretty easy to implement. I think it's such a very general purpose technology for a lot of workflows. This would give you a significant boost in the performance of LLMs. The second design pattern is tool use. Many of you will already have seen LLM-based systems using tools. On the left is a screenshot from copilot. On the right is something that I extracted from GPT-4. But LLMs today, if you ask it, what's the best coffee maker in your web search for some problems, LLMs will generate code and run codes. And it turns out that there are a lot of different tools that many different people are using for analysis, for gathering information, for taking action, for personal productivity. It turns out a lot of the early work in tool use turned out to be in the computer vision community because before large language models, LLMs, they couldn't do anything with images. So the only option was to generate a function call that could manipulate an image, like generate an image or do object detection or whatever. So if you actually look at literature, it's been interesting how much of the work in tool seems like it originated from vision because LLMs were blind to images before GPT-4V and Lava and so on. So that's tool use and it expands what an LLM can do. And then planning. For those of you that have not yet played a lot with planning algorithms, I feel like a lot of people talk about the chat GPT moment where you're, wow, never seen anything like this. I think you've not used planning algorithms. Many people will have a kind of AI agent, wow, I couldn't imagine an AI agent doing this. So I've run live demos where something failed and the AI agent rerouted around the failures. I've actually had quite a few of those moments where, wow, I can't believe my AI system just did that autonomously. But one example that I adapted from a hugging GPT paper, you say, please generate an image where a girl is reading a book and the pose is the same as the boy in the image, example.jpg And please describe the new image for the boy. So give an example like this. Today with AI agents, you can kind of decide, first thing I need to do is determine the pose of the boy. Then find the right model, maybe on hugging face, to extract the pose. Then next, you need to find the post image model to synthesize a picture of a girl as following the instructions. Then use image to text. And then finally use text to speech. And today we actually have agents that I don't want to say they work reliably. They're kind of finicky. They don't always work. But when it works, it's actually pretty amazing. But with agent groups, sometimes you can recover from earlier failures as well. So I find myself already using research agents for some of my work. Well, one piece of research, but I don't feel like Googling myself and spend a long time. I should send to the research agent, come back in a few minutes and see what it's come up with. And it sometimes works, sometimes it doesn't. But that's already part of my personal workflow. The final design pattern, multi-agent collaboration. This is one of those funny things. But it works much better than you might think. But on the left is a screenshot from a paper called Chat Dev, which is actually open source. Many of you saw the flashy social media announcements of demo of a devin. Chat Dev is open source, it runs on my laptop. And what Chat Dev does is example of a multi-agent system where you prompt one LLM to sometimes act like the CEO of a software engine company, sometimes act like a designer.[0m

[1m> Finished chain.[0m
[1m[92m [DEBUG]: == [Video Processor] Task output: All of you know Andrew Ng as a famous computer science professor at Stanford, was really early on in the development of neural networks with GPUs. Of course, a creator of Coursera and popular courses like deeplearning.ai, also the founder and creator and early lead of Google Brain. But one thing I've always wanted to ask you before I hand it over Andrew while you're on stage is a question I think would be relevant to the whole audience. Ten years ago on problem set number two of CS229, you gave me a B. And I was wondering, I looked it over, I was wondering what you saw that I did incorrectly. So anyway, Andrew. Thank you, Hansin. I'm looking forward to sharing with all of you what I'm seeing with AI agents, which I think is an exciting trend that I think everyone building in AI should pay attention to. And then also excited about all the other what's next presentations. So AI agents, today the way most of us use large language models is like this, with a non-agentic workflow where you type a prompt and generates an answer. And that's a bit like if you ask a person to write an essay on a topic and I say, please sit down at the keyboard and just type the essay from start to finish without ever using backspace. And despite how hard this is, LLMs do it remarkably well. In contrast with an agentic workflow, this is what it may look like. Have an AI, have an LLM, say write an essay outline. Do you need to do any web research? If so, let's do that. Then write the first draft and then read your own draft and think about what parts need revision and then revise your draft and you go on and on. And so this workflow is much more iterative where you may have the LLM do some thinking and then revise this article and then do some more thinking and iterate this through a number of times. And what not many people appreciate is this delivers remarkably better results. I've actually really surprised myself working with these agent workflows, how well they work. I want to do one case study. My team analyzed some data using a coding benchmark called the human eval benchmark released by OpenAI a few years ago. But this says coding problems like given the non-NT list of integers return the sum of all the odd elements or uneven positions. And it turns out the answer is code snippet like that. So today, a lot of us will use zero-shot prompting, meaning we tell the AI write the code and have it run on the first pass. Like who codes like that? No human codes like that. We just type out the code and run it. Maybe you do. But you can't do that. So it turns out that if you use GPT 3.5, zero-shot prompting, it gets it 48% right. GPT 4, way better, 67% right. But if you take an agent workflow and wrap it around GPT 3.5, say, it actually does better than even GPT 4. And if you were to wrap this type of workflow around GPT 4, it also does very well. And you notice that GPT 3.5 with an agent workflow actually outperforms GPT 4. And I think this means that this has significant consequences for how we all approach building applications. So agents is a term that has been tossed around a lot. There's a lot of consultant reports, how about agents, the future of AI, blah, blah, blah. I want to be a bit concrete and share with you the broad design patterns I'm seeing in agents. It's a very messy, chaotic space. Tons of research, tons of open source. There's a lot going on. But I try to categorize a bit more concretely what's going on in agents. Reflection is a tool that I think many of us should just use. It just works. Tool use, I think it's more widely appreciated. But it actually works pretty well. I think of these as pretty robust technologies. When I use them, I can almost always get them to work well. Planning and multi-agent collaboration, I think is more emerging. When I use them, sometimes my mind is blown for how well they work. But at least at this moment in time, I don't feel like I can always get them to work reliably. So let me walk through these four design patterns in a few slides. And if some of you go back and yourself will ask your engineers to use these, I think you get a productivity boost quite quickly. So reflection, here's an example. Let's say I ask a system, please write code for me for a given task. Then we have a coded agent, just an LLM that you prompt to write code to say, yo, def, do task, write a function like that. An example of self-reflection would be if you then prompt the LLM with something like this. Here's code intended for a task and just give it back the exact same code that you just generated. And then say, check the code carefully for correctness, sound, efficiency, good construction for them. Just write a prompt like that. It turns out the same LLM that you prompted to write the code may be able to spot problems like this bug in line five, we fix it, blah, blah, blah. And if you now take his own feedback and give it to it and reprompt it, it may come up with a version two of the code that could well work better than the first version. Not guaranteed, but it works often enough for this to be worth trying for a lot of applications. To foreshadow tool use, if you let it run unit tests, if it fails a unit test, then why did you fail the unit test? Have that conversation and maybe they'll figure out, fail the unit test, so you should try changing something and come up with v3. By the way, for those of you that want to learn more about these technologies, I'm very excited about them. For each of the four sections, I have a little recommended reading section in the bottom that hopefully gives more references. And again, just to foreshadow multi-agent systems, I've described as a single coder agent that you prompt to have this conversation with itself. One natural evolution of this idea is instead of a single coder agent, you can have two agents where one is a coder agent and the second is a critic agent. And these could be the same base LLM model, but you prompt in different ways. We say one, you're an expert coder, write code. The other one is say you're an expert code reviewer, review this code. And this type of workflow is actually pretty easy to implement. I think it's such a very general purpose technology for a lot of workflows. This would give you a significant boost in the performance of LLMs. The second design pattern is tool use. Many of you will already have seen LLM-based systems using tools. On the left is a screenshot from copilot. On the right is something that I extracted from GPT-4. But LLMs today, if you ask it, what's the best coffee maker in your web search for some problems, LLMs will generate code and run codes. And it turns out that there are a lot of different tools that many different people are using for analysis, for gathering information, for taking action, for personal productivity. It turns out a lot of the early work in tool use turned out to be in the computer vision community because before large language models, LLMs, they couldn't do anything with images. So the only option was to generate a function call that could manipulate an image, like generate an image or do object detection or whatever. So if you actually look at literature, it's been interesting how much of the work in tool seems like it originated from vision because LLMs were blind to images before GPT-4V and Lava and so on. So that's tool use and it expands what an LLM can do. And then planning. For those of you that have not yet played a lot with planning algorithms, I feel like a lot of people talk about the chat GPT moment where you're, wow, never seen anything like this. I think you've not used planning algorithms. Many people will have a kind of AI agent, wow, I couldn't imagine an AI agent doing this. So I've run live demos where something failed and the AI agent rerouted around the failures. I've actually had quite a few of those moments where, wow, I can't believe my AI system just did that autonomously. But one example that I adapted from a hugging GPT paper, you say, please generate an image where a girl is reading a book and the pose is the same as the boy in the image, example.jpg And please describe the new image for the boy. So give an example like this. Today with AI agents, you can kind of decide, first thing I need to do is determine the pose of the boy. Then find the right model, maybe on hugging face, to extract the pose. Then next, you need to find the post image model to synthesize a picture of a girl as following the instructions. Then use image to text. And then finally use text to speech. And today we actually have agents that I don't want to say they work reliably. They're kind of finicky. They don't always work. But when it works, it's actually pretty amazing. But with agent groups, sometimes you can recover from earlier failures as well. So I find myself already using research agents for some of my work. Well, one piece of research, but I don't feel like Googling myself and spend a long time. I should send to the research agent, come back in a few minutes and see what it's come up with. And it sometimes works, sometimes it doesn't. But that's already part of my personal workflow. The final design pattern, multi-agent collaboration. This is one of those funny things. But it works much better than you might think. But on the left is a screenshot from a paper called Chat Dev, which is actually open source. Many of you saw the flashy social media announcements of demo of a devin. Chat Dev is open source, it runs on my laptop. And what Chat Dev does is example of a multi-agent system where you prompt one LLM to sometimes act like the CEO of a software engine company, sometimes act like a designer.

2024-06-19 14:10:37,515 - 140573001590592 - local_persistent_hnsw.py-local_persistent_hnsw:347 - WARNING: Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2
[00m
[1m[95m [DEBUG]: == Working Agent: Text Summarizer[00m
[1m[95m [INFO]: == Starting Task: Using RAG, summarize the text. Ensure the summary captures the key points and relevant details from the content. Finally, produce a well-organized and comprehensive article.
use TXTSearchTool tool rag  
If there are a lot of repetitions, only the meaningful text is summarized.(For example, the key words and their meanings)  All steps are in English
file_path:  '/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt'
[00m


[1m> Entering new CrewAgentExecutor chain...[0m
[32;1m[1;3mThe task requires me to summarize a lengthy transcript from an audio file that has been converted into text using the Whisper model. The text seems to be a talk given by Andrew Ng about Large Language Models (LLMs), their use, and the future of AI. To get the main points and relevant details, I'll use the tool 'Search a txt's content' to find key points and themes in the text.

Action: Search a txt's content
Action Input: {"search_query": "key points", "txt": "/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/output/Andrew_agent.txt"}[0m[95m 

Relevant Content:
真是太棒了我沒想到AI會這麼好我做了一些現場的示範當中有些東西失敗了AI會重新輸入失敗的部分我其實有很多時候會覺得哇 我的AI系統我真的不能相信我的AI系統只是這樣自行調整但我從GPT上用了一個例子來自我學習的請你給女生在讀書時發出一張圖片然後把圖片跟男生一樣例如jpeg請你寫下女生在讀的圖片所以像這樣今天AI人士可以決定首先要決定男生的姿勢然後找出對的模式可能是抱抱的然後要把姿勢取出然後下一個要找出對的模式來把女生的圖片作為指示來做進行然後用來寫圖片然後最後用來寫言語今天我們有些AI人士我不會說他們很實際他們很隨意他們不常用但當他們用的時候其實還蠻有趣的但用AI人士來做有時候可以重新復發從前的失敗我自己已經找到有些人用資料人員在我工作上我只是一份資料人員但我沒有感覺到自己在搜尋花了很長時間我只會把資料人員送到資料人員在幾分鐘內再看看有什麼可以做有時候可以有時候不可以但這已經是我的個人工作最後的設計模式是多人人合作這是一個有趣的東西但它比你可能想像的更好但它比你可能想像的更好但左邊是一張圖片叫做Chatdev是完全開放的是開放的很多人都看到Demo的社交媒體的Demonstration的DemonstrationChatdev是開放的它在我的電腦上開放Chatdev是一個多人人合作的系統你會提供一種LM在某個時候像是一個軟體工程機的經理人或是一個軟體工程機的經理人或是一個軟體工程機的經理人或是一個軟體工程機的經理人或是一個軟體工程機的經理人而這堆的經理人你建立的你會提供LM告訴他們你現在是一個軟體工程機的經理人你現在是一個軟體工程機的經理人他們會合作有一個廣泛的交流如果你們告訴他們請發展一個遊戲發展一個GoMoki遊戲他們會在幾分鐘內寫文章測試編輯然後發展像是非常複雜的程式不一定會成功我用過了有時候不成功有時候很厲害但這技術真的在變得更好而一個設計的模式就像是多個經理人的討論有不同的經理人例如有ChaiGPD還有Gemini討論著這其實也會有更好的表現所以有很多類似的經理人一起合作也有很強大的設計模式所以說起來總結來說這些是我們所見的模式如果我們用這些模式我們很多人都能夠得到很快的實力我認為Agentic的設計模式是非常重要的這是我的小圖我預計Agentic的設計模式在今年會大幅地增長因為Agentic的工作有些人很難熟悉的事就是我們在LM上發表訊息我們會馬上回應在一年多前我去Google發表訊息我們叫做大搜尋我們會發出長期訊息我失敗的原因是因為當你發表搜尋你會回應不到一半這是人類的方式我們喜歡那種即時的訊息但在很多Agentic的工作我認為我們需要學習如何提供Agentic的訊息讓人們用心地等待可能甚至等待一小時讓人們回應但我看到很多新手提供一些訊息給別人然後再檢查五分鐘後這並不成功我認為這會很困難我們需要與Agentic的一些人一起做這件事我失敗了另外,另一個重要的步驟是快速的訊息在Agentic的工作我們在每個部分都在編繩所以LM會在讀取訊息的速度上編繩所以能夠在讀取訊息的速度上編繩比任何人都要快這很棒我認為編繩的速度會很快即使LM的質量是低也會有好的成果相比起更慢的LM可能會有一點點困難因為可能會讓你在這段路上很多多次走我認為我所展示的結果是GPD3和Agent的建築在第一張圖表上而Candidly我真的期待Clar 4、GPD5、Gemini 2.0以及其他一般的一般的模式在Mimpio建築而我認為如果你想要在GPD5上發展你可能會比較接近那種程度的表現或是一些應用程式你可能會想到在Agentic的訊息上而不是在早期模式上我認為這是一個重要的趨勢而坦白說AGI的路線感覺像是一種旅程不像是一個地點但我覺得這類Agentic的工作可以幫助我們在這段很長的旅程進一步前進謝謝

所有的人都知道Andrew Ng是一位名叫Stanford的科技科學教授他在GPU中發展的一段時間很早當然他也是一位創作者在Coursera上也有很多著名的科目,例如DeepLearning.AI他也是Google Brain的創作者和創作者但我一直想問你,在你上台的時候這問題對整個觀眾來說是很重要的十年前,在C2.29的《問題點2》中你給了我一筆我看了一遍,我問了你看得懂我做的什麼Anyway, Andrew謝謝,昆西我希望能和你們分享我對AI的專業人員的觀點我認為是一個很興奮的趨勢我認為所有人在建立AI的工作都應該要留意我還很期待接下來的演講今天,我們大多數用大型的語言模式是這樣的我們有一個不用語言的工作你點了一個題目,然後它會發出答案這就像是你問一個人寫了一篇文章我會說,請坐下來,按鍵盤然後把文章從開始到終點寫出來並不再用背景的方式寫出來就算多麼艱難,LOMs做得非常好相比起一個人的工作這就是它們的樣子有一個AI、LOM寫一篇文章你需要做任何的資料搜集嗎?如果是,那就做吧然後寫第一篇文章然後寫自己的第一篇文章然後想想要什麼部分需要修改然後再寫第二篇文章然後你繼續寫所以這個工作是一種方式我會說,請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來請坐下來然後是計劃那些還沒玩過計劃的玩家我感覺很多人在聊聊GPT時會覺得哇 從來沒見過這樣如果沒有用計劃的玩家很多人會覺得哇
2024-06-19 14:10:37,831 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 0.3s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:10:38,136 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.9s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
[00m
Traceback (most recent call last):
  File "<string>", line 1, in <module>
RuntimeError: Dora Runtime raised an error.

Caused by:
   0: main task failed
   1: operator run/op raised an error
   2: error in Python module at /mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/agent.py
   3: Traceback (most recent call last):
        File "/mnt/d/project/zzbc/experiment_project/experiment_project/moxin/crewai/agent.py", line 20, in on_event
          result = run_crewai(crewai_config=agent_config)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/mnt/d/project/zzbc/experiment_project/experiment_project/agents/crewai_module/run_task.py", line 55, in run_crewai
          result = crew.kickoff()
                   ^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/crew.py", line 264, in kickoff
          result = self._run_sequential_process()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/crew.py", line 305, in _run_sequential_process
          output = task.execute(context=task_output)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/task.py", line 183, in execute
          result = self._execute(
                   ^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/task.py", line 192, in _execute
          result = agent.execute_task(
                   ^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/agent.py", line 236, in execute_task
          result = self.agent_executor.invoke(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain/chains/base.py", line 163, in invoke
          raise e
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain/chains/base.py", line 153, in invoke
          self._call(inputs, run_manager=run_manager)
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/agents/executor.py", line 128, in _call
          next_step_output = self._take_next_step(
                             ^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain/agents/agent.py", line 1138, in _take_next_step
          [
        File "/home/cc/miniconda/lib/python3.12/site-packages/crewai/agents/executor.py", line 192, in _iter_next_step
          output = self.agent.plan(  # type: ignore #  Incompatible types in assignment (expression has type "AgentAction | AgentFinish | list[AgentAction]", variable has type "AgentAction")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain/agents/agent.py", line 397, in plan
          for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 2875, in stream
          yield from self.transform(iter([input]), config, **kwargs)
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 2862, in transform
          yield from self._transform_stream_with_config(
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 1881, in _transform_stream_with_config
          chunk: Output = context.run(next, iterator)  # type: ignore
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 2826, in _transform
          for output in final_pipeline:
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 1282, in transform
          for ichunk in input:
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 4736, in transform
          yield from self.bound.transform(
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 1300, in transform
          yield from self.stream(final, config, **kwargs)
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 249, in stream
          raise e
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 229, in stream
          for chunk in self._stream(messages, stop=stop, **kwargs):
        File "/home/cc/miniconda/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 408, in _stream
          for chunk in self.client.create(messages=message_dicts, **params):
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/openai/_utils/_utils.py", line 275, in wrapper
          return func(*args, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 667, in create
          return self._post(
                 ^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/openai/_base_client.py", line 1213, in post
          return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/openai/_base_client.py", line 902, in request
          return self._request(
                 ^^^^^^^^^^^^^^
        File "/home/cc/miniconda/lib/python3.12/site-packages/openai/_base_client.py", line 993, in _request
          raise self._make_status_error_from_response(err.response) from None

      BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, your messages resulted in 8534 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}

Location:
    binaries/runtime/src/operator/python.rs:28:9
2024-06-19 14:10:40,042 - 140568006993472 - _common.py-_common:105 - INFO: Backing off send_request(...) for 1.2s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
2024-06-19 14:10:41,216 - 140568006993472 - _common.py-_common:120 - ERROR: Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us-api.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))
