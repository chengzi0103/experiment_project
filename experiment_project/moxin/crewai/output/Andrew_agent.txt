 All of you know Andrew Ng as a famous computer science professor at Stanford, was really early on in the development of neural networks with GPUs. Of course, a creator of Coursera and popular courses like deeplearning.ai, also the founder and creator and early lead of Google Brain. But one thing I've always wanted to ask you before I hand it over Andrew while you're on stage is a question I think would be relevant to the whole audience. Ten years ago on problem set number two of CS229, you gave me a B. And I was wondering, I looked it over, I was wondering what you saw that I did incorrectly. So anyway, Andrew. Thank you, Hansin. I'm looking forward to sharing with all of you what I'm seeing with AI agents, which I think is an exciting trend that I think everyone building in AI should pay attention to. And then also excited about all the other what's next presentations. So AI agents, today the way most of us use large language models is like this, with a non-agentic workflow where you type a prompt and generates an answer. And that's a bit like if you ask a person to write an essay on a topic and I say, please sit down at the keyboard and just type the essay from start to finish without ever using backspace. And despite how hard this is, LLMs do it remarkably well. In contrast with an agentic workflow, this is what it may look like. Have an AI, have an LLM, say write an essay outline. Do you need to do any web research? If so, let's do that. Then write the first draft and then read your own draft and think about what parts need revision and then revise your draft and you go on and on. And so this workflow is much more iterative where you may have the LLM do some thinking and then revise this article and then do some more thinking and iterate this through a number of times. And what not many people appreciate is this delivers remarkably better results. I've actually really surprised myself working with these agent workflows, how well they work. I want to do one case study. My team analyzed some data using a coding benchmark called the human eval benchmark released by OpenAI a few years ago. But this says coding problems like given the non-NT list of integers return the sum of all the odd elements or uneven positions. And it turns out the answer is code snippet like that. So today, a lot of us will use zero-shot prompting, meaning we tell the AI write the code and have it run on the first pass. Like who codes like that? No human codes like that. We just type out the code and run it. Maybe you do. But you can't do that. So it turns out that if you use GPT 3.5, zero-shot prompting, it gets it 48% right. GPT 4, way better, 67% right. But if you take an agent workflow and wrap it around GPT 3.5, say, it actually does better than even GPT 4. And if you were to wrap this type of workflow around GPT 4, it also does very well. And you notice that GPT 3.5 with an agent workflow actually outperforms GPT 4. And I think this means that this has significant consequences for how we all approach building applications. So agents is a term that has been tossed around a lot. There's a lot of consultant reports, how about agents, the future of AI, blah, blah, blah. I want to be a bit concrete and share with you the broad design patterns I'm seeing in agents. It's a very messy, chaotic space. Tons of research, tons of open source. There's a lot going on. But I try to categorize a bit more concretely what's going on in agents. Reflection is a tool that I think many of us should just use. It just works. Tool use, I think it's more widely appreciated. But it actually works pretty well. I think of these as pretty robust technologies. When I use them, I can almost always get them to work well. Planning and multi-agent collaboration, I think is more emerging. When I use them, sometimes my mind is blown for how well they work. But at least at this moment in time, I don't feel like I can always get them to work reliably. So let me walk through these four design patterns in a few slides. And if some of you go back and yourself will ask your engineers to use these, I think you get a productivity boost quite quickly. So reflection, here's an example. Let's say I ask a system, please write code for me for a given task. Then we have a coded agent, just an LLM that you prompt to write code to say, yo, def, do task, write a function like that. An example of self-reflection would be if you then prompt the LLM with something like this. Here's code intended for a task and just give it back the exact same code that you just generated. And then say, check the code carefully for correctness, sound, efficiency, good construction for them. Just write a prompt like that. It turns out the same LLM that you prompted to write the code may be able to spot problems like this bug in line five, we fix it, blah, blah, blah. And if you now take his own feedback and give it to it and reprompt it, it may come up with a version two of the code that could well work better than the first version. Not guaranteed, but it works often enough for this to be worth trying for a lot of applications. To foreshadow tool use, if you let it run unit tests, if it fails a unit test, then why did you fail the unit test? Have that conversation and maybe they'll figure out, fail the unit test, so you should try changing something and come up with v3. By the way, for those of you that want to learn more about these technologies, I'm very excited about them. For each of the four sections, I have a little recommended reading section in the bottom that hopefully gives more references. And again, just to foreshadow multi-agent systems, I've described as a single coder agent that you prompt to have this conversation with itself. One natural evolution of this idea is instead of a single coder agent, you can have two agents where one is a coder agent and the second is a critic agent. And these could be the same base LLM model, but you prompt in different ways. We say one, you're an expert coder, write code. The other one is say you're an expert code reviewer, review this code. And this type of workflow is actually pretty easy to implement. I think it's such a very general purpose technology for a lot of workflows. This would give you a significant boost in the performance of LLMs. The second design pattern is tool use. Many of you will already have seen LLM-based systems using tools. On the left is a screenshot from copilot. On the right is something that I extracted from GPT-4. But LLMs today, if you ask it, what's the best coffee maker in your web search for some problems, LLMs will generate code and run codes. And it turns out that there are a lot of different tools that many different people are using for analysis, for gathering information, for taking action, for personal productivity. It turns out a lot of the early work in tool use turned out to be in the computer vision community because before large language models, LLMs, they couldn't do anything with images. So the only option was to generate a function call that could manipulate an image, like generate an image or do object detection or whatever. So if you actually look at literature, it's been interesting how much of the work in tool seems like it originated from vision because LLMs were blind to images before GPT-4V and Lava and so on. So that's tool use and it expands what an LLM can do. And then planning. For those of you that have not yet played a lot with planning algorithms, I feel like a lot of people talk about the chat GPT moment where you're, wow, never seen anything like this. I think you've not used planning algorithms. Many people will have a kind of AI agent, wow, I couldn't imagine an AI agent doing this. So I've run live demos where something failed and the AI agent rerouted around the failures. I've actually had quite a few of those moments where, wow, I can't believe my AI system just did that autonomously. But one example that I adapted from a hugging GPT paper, you say, please generate an image where a girl is reading a book and the pose is the same as the boy in the image, example.jpg And please describe the new image for the boy. So give an example like this. Today with AI agents, you can kind of decide, first thing I need to do is determine the pose of the boy. Then find the right model, maybe on hugging face, to extract the pose. Then next, you need to find the post image model to synthesize a picture of a girl as following the instructions. Then use image to text. And then finally use text to speech. And today we actually have agents that I don't want to say they work reliably. They're kind of finicky. They don't always work. But when it works, it's actually pretty amazing. But with agent groups, sometimes you can recover from earlier failures as well. So I find myself already using research agents for some of my work. Well, one piece of research, but I don't feel like Googling myself and spend a long time. I should send to the research agent, come back in a few minutes and see what it's come up with. And it sometimes works, sometimes it doesn't. But that's already part of my personal workflow. The final design pattern, multi-agent collaboration. This is one of those funny things. But it works much better than you might think. But on the left is a screenshot from a paper called Chat Dev, which is actually open source. Many of you saw the flashy social media announcements of demo of a devin. Chat Dev is open source, it runs on my laptop. And what Chat Dev does is example of a multi-agent system where you prompt one LLM to sometimes act like the CEO of a software engine company, sometimes act like a designer.